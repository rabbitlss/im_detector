# -*- coding: utf-8 -*-
"""
OCRæ€§èƒ½ä¼˜åŒ–æŠ€å·§
åŒ…å«ç¼“å­˜ã€æ‰¹å¤„ç†ã€å¹¶è¡Œç­‰ä¼˜åŒ–æ–¹æ³•
"""

import cv2
import numpy as np
import hashlib
import time
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from functools import lru_cache
from collections import OrderedDict
from ultra_fast_ocr import UltraFastOCR


class OptimizedOCR:
    """ä¼˜åŒ–åçš„OCRï¼ˆç¨³å®š3-5msï¼‰"""
    
    def __init__(self, cache_size: int = 1000):
        """
        åˆå§‹åŒ–ä¼˜åŒ–OCR
        
        Args:
            cache_size: ç¼“å­˜å¤§å°
        """
        self.ocr = UltraFastOCR(use_gpu=True)
        
        # LRUç¼“å­˜
        self.cache = OrderedDict()
        self.cache_size = cache_size
        self.cache_hits = 0
        self.cache_misses = 0
        
        # é¢„ç¼–è¯‘çš„æ¨¡æ¿
        self.templates = {
            'å‘é€': self._create_template('å‘é€'),
            'Send': self._create_template('Send'),
            'ç¡®å®š': self._create_template('ç¡®å®š'),
            'å–æ¶ˆ': self._create_template('å–æ¶ˆ'),
        }
    
    def _create_template(self, text: str) -> np.ndarray:
        """åˆ›å»ºæ–‡å­—æ¨¡æ¿ç”¨äºå¿«é€ŸåŒ¹é…"""
        img = np.ones((48, len(text) * 30, 3), dtype=np.uint8) * 255
        cv2.putText(img, text, (10, 35), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
        return img
    
    def get_image_hash(self, image: np.ndarray) -> str:
        """è®¡ç®—å›¾ç‰‡å“ˆå¸Œï¼ˆç”¨äºç¼“å­˜ï¼‰"""
        # ç¼©å°å›¾ç‰‡ä»¥åŠ é€Ÿå“ˆå¸Œè®¡ç®—
        small = cv2.resize(image, (32, 32))
        return hashlib.md5(small.tobytes()).hexdigest()
    
    def recognize_with_cache(self, image_region: np.ndarray) -> Tuple[str, bool]:
        """
        å¸¦ç¼“å­˜çš„è¯†åˆ«ï¼ˆé‡å¤å†…å®¹0msï¼‰
        
        Args:
            image_region: å›¾ç‰‡åŒºåŸŸ
            
        Returns:
            (è¯†åˆ«çš„æ–‡å­—, æ˜¯å¦ç¼“å­˜å‘½ä¸­)
        """
        # è®¡ç®—å“ˆå¸Œ
        img_hash = self.get_image_hash(image_region)
        
        # æ£€æŸ¥ç¼“å­˜
        if img_hash in self.cache:
            self.cache_hits += 1
            # ç§»åˆ°æœ€åï¼ˆLRUï¼‰
            self.cache.move_to_end(img_hash)
            return self.cache[img_hash], True
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œè¿›è¡ŒOCR
        self.cache_misses += 1
        text, _, _ = self.ocr.recognize_single_line(image_region)
        
        # æ·»åŠ åˆ°ç¼“å­˜
        self.cache[img_hash] = text
        
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.cache) > self.cache_size:
            # åˆ é™¤æœ€æ—§çš„é¡¹
            self.cache.popitem(last=False)
        
        return text, False
    
    def batch_recognize(self, image_regions: List[np.ndarray], 
                       use_parallel: bool = True) -> List[str]:
        """
        æ‰¹é‡è¯†åˆ«ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
        
        Args:
            image_regions: å›¾ç‰‡åŒºåŸŸåˆ—è¡¨
            use_parallel: æ˜¯å¦ä½¿ç”¨å¹¶è¡Œ
            
        Returns:
            è¯†åˆ«çš„æ–‡å­—åˆ—è¡¨
        """
        if not use_parallel or len(image_regions) < 4:
            # ä¸²è¡Œå¤„ç†
            results = []
            for region in image_regions:
                text, _ = self.recognize_with_cache(region)
                results.append(text)
            return results
        
        # å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(self.recognize_with_cache, region) 
                      for region in image_regions]
            results = [future.result()[0] for future in futures]
        
        return results
    
    def recognize_im_element(self, image: np.ndarray, 
                           element_type: str, 
                           bbox: List[int]) -> str:
        """
        é’ˆå¯¹IMå…ƒç´ ä¼˜åŒ–çš„è¯†åˆ«
        
        Args:
            image: å®Œæ•´å›¾ç‰‡
            element_type: å…ƒç´ ç±»å‹
            bbox: è¾¹ç•Œæ¡† [x1, y1, x2, y2]
            
        Returns:
            è¯†åˆ«çš„æ–‡å­—
        """
        x1, y1, x2, y2 = bbox
        roi = image[y1:y2, x1:x2]
        
        # æ ¹æ®å…ƒç´ ç±»å‹ä¼˜åŒ–
        if element_type == 'send_button':
            # å‘é€æŒ‰é’®é€šå¸¸æ˜¯å›ºå®šæ–‡å­—ï¼Œå…ˆå°è¯•æ¨¡æ¿åŒ¹é…
            for template_text, template_img in self.templates.items():
                if self._quick_match(roi, template_img):
                    return template_text
            # æ¨¡æ¿åŒ¹é…å¤±è´¥ï¼Œä½¿ç”¨OCR
            return self.recognize_with_cache(roi)[0]
            
        elif element_type == 'receiver_name':
            # æ˜µç§°é€šå¸¸æ˜¯å•è¡Œï¼Œå¯èƒ½æœ‰è¡¨æƒ…ç¬¦å·
            text, _ = self.recognize_with_cache(roi)
            return text
            
        elif element_type == 'chat_message':
            # æ¶ˆæ¯å¯èƒ½å¤šè¡Œï¼Œä½¿ç”¨å¤šè¡Œè¯†åˆ«
            return self.ocr.recognize(roi, single_line=False)
            
        elif element_type == 'input_box':
            # è¾“å…¥æ¡†å¯èƒ½æ˜¯ç©ºçš„æˆ–æœ‰æç¤ºæ–‡å­—
            if self._is_empty_input(roi):
                return ""
            return self.recognize_with_cache(roi)[0]
        
        else:
            return self.recognize_with_cache(roi)[0]
    
    def _quick_match(self, roi: np.ndarray, template: np.ndarray) -> bool:
        """å¿«é€Ÿæ¨¡æ¿åŒ¹é…ï¼ˆ1msï¼‰"""
        try:
            # è°ƒæ•´å¤§å°
            roi_resized = cv2.resize(roi, (template.shape[1], template.shape[0]))
            # è®¡ç®—ç›¸ä¼¼åº¦
            diff = cv2.absdiff(roi_resized, template)
            score = np.mean(diff)
            return score < 30  # é˜ˆå€¼
        except:
            return False
    
    def _is_empty_input(self, roi: np.ndarray) -> bool:
        """åˆ¤æ–­è¾“å…¥æ¡†æ˜¯å¦ä¸ºç©º"""
        # æ£€æŸ¥æ˜¯å¦ä¸»è¦æ˜¯ç™½è‰²/æµ…è‰²
        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
        mean_val = np.mean(gray)
        return mean_val > 240  # ä¸»è¦æ˜¯ç™½è‰²
    
    def get_statistics(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0
        
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate,
            'cache_size': len(self.cache),
            'total_requests': total_requests
        }


class ParallelOCR:
    """å¹¶è¡ŒOCRå¤„ç†å™¨"""
    
    def __init__(self, num_workers: int = 4):
        """
        åˆå§‹åŒ–å¹¶è¡Œå¤„ç†å™¨
        
        Args:
            num_workers: å·¥ä½œçº¿ç¨‹æ•°
        """
        self.num_workers = num_workers
        self.ocr_pool = [UltraFastOCR(use_gpu=True) for _ in range(num_workers)]
    
    def process_batch(self, images: List[np.ndarray], 
                     batch_size: int = 10) -> List[str]:
        """
        æ‰¹é‡å¹¶è¡Œå¤„ç†
        
        Args:
            images: å›¾ç‰‡åˆ—è¡¨
            batch_size: æ¯æ‰¹å¤§å°
            
        Returns:
            è¯†åˆ«ç»“æœåˆ—è¡¨
        """
        results = []
        
        for i in range(0, len(images), batch_size):
            batch = images[i:i + batch_size]
            
            with ThreadPoolExecutor(max_workers=self.num_workers) as executor:
                # åˆ†é…OCRå®ä¾‹
                futures = []
                for j, img in enumerate(batch):
                    ocr = self.ocr_pool[j % self.num_workers]
                    future = executor.submit(ocr.recognize_single_line, img)
                    futures.append(future)
                
                # æ”¶é›†ç»“æœ
                for future in futures:
                    text, _, _ = future.result()
                    results.append(text)
        
        return results


class StreamingOCR:
    """æµå¼OCRå¤„ç†å™¨ï¼ˆç”¨äºè§†é¢‘/å®æ—¶æµï¼‰"""
    
    def __init__(self, buffer_size: int = 30):
        """
        åˆå§‹åŒ–æµå¼å¤„ç†å™¨
        
        Args:
            buffer_size: ç¼“å†²åŒºå¤§å°
        """
        self.ocr = OptimizedOCR()
        self.buffer_size = buffer_size
        self.frame_buffer = []
        self.result_buffer = []
        
    def process_frame(self, frame: np.ndarray, 
                     regions: List[Dict]) -> List[Dict]:
        """
        å¤„ç†å•å¸§
        
        Args:
            frame: è§†é¢‘å¸§
            regions: éœ€è¦OCRçš„åŒºåŸŸåˆ—è¡¨
                [{'bbox': [x1,y1,x2,y2], 'type': 'chat_message'}, ...]
                
        Returns:
            OCRç»“æœåˆ—è¡¨
        """
        results = []
        
        for region in regions:
            bbox = region['bbox']
            element_type = region.get('type', 'unknown')
            
            # æå–æ–‡å­—
            text = self.ocr.recognize_im_element(frame, element_type, bbox)
            
            results.append({
                'bbox': bbox,
                'type': element_type,
                'text': text,
                'timestamp': time.time()
            })
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        self.result_buffer.append(results)
        if len(self.result_buffer) > self.buffer_size:
            self.result_buffer.pop(0)
        
        return results
    
    def get_stable_text(self, bbox: List[int], 
                       min_occurrences: int = 3) -> Optional[str]:
        """
        è·å–ç¨³å®šçš„æ–‡å­—ï¼ˆå¤šå¸§ä¸­å‡ºç°çš„ï¼‰
        
        Args:
            bbox: è¾¹ç•Œæ¡†
            min_occurrences: æœ€å°å‡ºç°æ¬¡æ•°
            
        Returns:
            ç¨³å®šçš„æ–‡å­—æˆ–None
        """
        texts = []
        
        for frame_results in self.result_buffer[-10:]:  # æ£€æŸ¥æœ€è¿‘10å¸§
            for result in frame_results:
                if self._bbox_overlap(result['bbox'], bbox) > 0.8:
                    texts.append(result['text'])
        
        # ç»Ÿè®¡å‡ºç°æ¬¡æ•°
        from collections import Counter
        text_counts = Counter(texts)
        
        for text, count in text_counts.most_common(1):
            if count >= min_occurrences:
                return text
        
        return None
    
    def _bbox_overlap(self, bbox1: List[int], bbox2: List[int]) -> float:
        """è®¡ç®—è¾¹ç•Œæ¡†é‡å åº¦ï¼ˆIoUï¼‰"""
        x1 = max(bbox1[0], bbox2[0])
        y1 = max(bbox1[1], bbox2[1])
        x2 = min(bbox1[2], bbox2[2])
        y2 = min(bbox1[3], bbox2[3])
        
        if x2 < x1 or y2 < y1:
            return 0.0
        
        intersection = (x2 - x1) * (y2 - y1)
        area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
        area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0


def benchmark_optimizations():
    """æµ‹è¯•å„ç§ä¼˜åŒ–æ•ˆæœ"""
    
    print("ğŸ”¬ OCRä¼˜åŒ–æ•ˆæœæµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_images = []
    for i in range(20):
        img = np.ones((48, 200, 3), dtype=np.uint8) * 255
        text = f"Test {i % 5}"  # æœ‰é‡å¤çš„æ–‡å­—
        cv2.putText(img, text, (20, 35),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
        test_images.append(img)
    
    # 1. åŸºç¡€OCRï¼ˆæ— ä¼˜åŒ–ï¼‰
    print("\n1. åŸºç¡€OCRï¼ˆæ— ä¼˜åŒ–ï¼‰:")
    basic_ocr = UltraFastOCR()
    start = time.time()
    for img in test_images:
        _ = basic_ocr.recognize_single_line(img)
    basic_time = time.time() - start
    print(f"   æ€»è€—æ—¶: {basic_time*1000:.1f}ms")
    print(f"   å¹³å‡: {basic_time*1000/len(test_images):.1f}ms/image")
    
    # 2. å¸¦ç¼“å­˜çš„OCR
    print("\n2. å¸¦ç¼“å­˜çš„OCR:")
    cached_ocr = OptimizedOCR()
    start = time.time()
    for img in test_images:
        _ = cached_ocr.recognize_with_cache(img)
    cached_time = time.time() - start
    stats = cached_ocr.get_statistics()
    print(f"   æ€»è€—æ—¶: {cached_time*1000:.1f}ms")
    print(f"   å¹³å‡: {cached_time*1000/len(test_images):.1f}ms/image")
    print(f"   ç¼“å­˜å‘½ä¸­ç‡: {stats['hit_rate']*100:.1f}%")
    print(f"   åŠ é€Ÿæ¯”: {basic_time/cached_time:.1f}x")
    
    # 3. æ‰¹é‡å¹¶è¡Œå¤„ç†
    print("\n3. æ‰¹é‡å¹¶è¡Œå¤„ç†:")
    cached_ocr_batch = OptimizedOCR()
    start = time.time()
    _ = cached_ocr_batch.batch_recognize(test_images, use_parallel=True)
    parallel_time = time.time() - start
    print(f"   æ€»è€—æ—¶: {parallel_time*1000:.1f}ms")
    print(f"   å¹³å‡: {parallel_time*1000/len(test_images):.1f}ms/image")
    print(f"   åŠ é€Ÿæ¯”: {basic_time/parallel_time:.1f}x")
    
    # 4. å¤šå®ä¾‹å¹¶è¡Œ
    print("\n4. å¤šå®ä¾‹å¹¶è¡Œ:")
    parallel_ocr = ParallelOCR(num_workers=4)
    start = time.time()
    _ = parallel_ocr.process_batch(test_images)
    multi_time = time.time() - start
    print(f"   æ€»è€—æ—¶: {multi_time*1000:.1f}ms")
    print(f"   å¹³å‡: {multi_time*1000/len(test_images):.1f}ms/image")
    print(f"   åŠ é€Ÿæ¯”: {basic_time/multi_time:.1f}x")
    
    print("\n" + "=" * 50)
    print("ğŸ“Š ä¼˜åŒ–æ•ˆæœæ€»ç»“:")
    print(f"   åŸºç¡€æ–¹æ¡ˆ: {basic_time*1000:.1f}ms")
    print(f"   ç¼“å­˜ä¼˜åŒ–: {cached_time*1000:.1f}ms ({basic_time/cached_time:.1f}x)")
    print(f"   å¹¶è¡Œä¼˜åŒ–: {parallel_time*1000:.1f}ms ({basic_time/parallel_time:.1f}x)")
    print(f"   å¤šå®ä¾‹: {multi_time*1000:.1f}ms ({basic_time/multi_time:.1f}x)")


def demo_im_optimization():
    """æ¼”ç¤ºIMåœºæ™¯çš„ä¼˜åŒ–"""
    
    print("\nğŸ’¬ IMåœºæ™¯OCRä¼˜åŒ–æ¼”ç¤º")
    print("=" * 50)
    
    # åˆå§‹åŒ–ä¼˜åŒ–OCR
    ocr = OptimizedOCR()
    
    # æ¨¡æ‹ŸIMç•Œé¢å…ƒç´ 
    elements = [
        {'type': 'receiver_name', 'text': 'å¼ ä¸‰', 'bbox': [150, 20, 250, 50]},
        {'type': 'chat_message', 'text': 'ä½ å¥½', 'bbox': [50, 100, 150, 140]},
        {'type': 'chat_message', 'text': 'åœ¨å—ï¼Ÿ', 'bbox': [50, 150, 150, 190]},
        {'type': 'send_button', 'text': 'å‘é€', 'bbox': [400, 500, 450, 530]},
        {'type': 'input_box', 'text': '', 'bbox': [50, 500, 390, 530]},
    ]
    
    # åˆ›å»ºæ¨¡æ‹Ÿå›¾ç‰‡
    img = np.ones((600, 500, 3), dtype=np.uint8) * 240
    
    for element in elements:
        x1, y1, x2, y2 = element['bbox']
        # ç»˜åˆ¶å…ƒç´ 
        if element['text']:
            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 255, 255), -1)
            cv2.putText(img, element['text'], (x1+10, y1+25),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)
    
    # è¯†åˆ«æµ‹è¯•
    print("\nè¯†åˆ«ç»“æœ:")
    total_time = 0
    
    for element in elements:
        start = time.time()
        text = ocr.recognize_im_element(img, element['type'], element['bbox'])
        elapsed = (time.time() - start) * 1000
        total_time += elapsed
        
        print(f"  {element['type']:15s}: '{text}' ({elapsed:.1f}ms)")
    
    print(f"\næ€»è€—æ—¶: {total_time:.1f}ms")
    print(f"å¹³å‡: {total_time/len(elements):.1f}ms/element")
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    stats = ocr.get_statistics()
    print(f"\nç¼“å­˜ç»Ÿè®¡:")
    print(f"  å‘½ä¸­: {stats['cache_hits']}")
    print(f"  æœªå‘½ä¸­: {stats['cache_misses']}")
    print(f"  å‘½ä¸­ç‡: {stats['hit_rate']*100:.1f}%")


if __name__ == "__main__":
    print("=" * 60)
    print("OCRæ€§èƒ½ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # 1. æµ‹è¯•å„ç§ä¼˜åŒ–
    benchmark_optimizations()
    
    # 2. IMåœºæ™¯ä¼˜åŒ–æ¼”ç¤º
    demo_im_optimization()
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
    print("1. ä½¿ç”¨ç¼“å­˜ï¼šç›¸åŒå†…å®¹0ms")
    print("2. æ‰¹é‡å¤„ç†ï¼šå‡å°‘å¼€é”€")
    print("3. å¹¶è¡Œå¤„ç†ï¼šå¤šæ ¸åŠ é€Ÿ")
    print("4. æ¨¡æ¿åŒ¹é…ï¼šå›ºå®šæ–‡å­—1ms")
    print("5. é¢„å¤„ç†ä¼˜åŒ–ï¼šäºŒå€¼åŒ–ã€å»å™ªç­‰")
    print("=" * 60)
