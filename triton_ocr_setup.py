#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Triton Server OCRéƒ¨ç½²é…ç½®
å°†UltraFastOCRæ¨¡å‹éƒ¨ç½²åˆ°NVIDIA Triton Server
"""

import os
import json
import numpy as np
from pathlib import Path

def create_triton_config():
    """åˆ›å»ºTritonæ¨¡å‹é…ç½®æ–‡ä»¶"""
    
    config = """
name: "ocr_model"
platform: "python"
max_batch_size: 32
input [
  {
    name: "image"
    data_type: TYPE_UINT8
    dims: [-1, -1, 3]
  }
]
output [
  {
    name: "text"
    data_type: TYPE_STRING
    dims: [1]
  }
]

# å®ä¾‹ç»„é…ç½® - æ¯ä¸ªGPUä¸€ä¸ªå®ä¾‹
instance_group [
  {
    count: 1
    kind: KIND_GPU
    gpus: [0]
  },
  {
    count: 1
    kind: KIND_GPU
    gpus: [1]
  }
]

# åŠ¨æ€æ‰¹å¤„ç†
dynamic_batching {
  max_queue_delay_microseconds: 100
}

# æ¨¡å‹é¢„çƒ­
model_warmup [
  {
    name: "warmup"
    batch_size: 1
    inputs: {
      "image": {
        data_type: TYPE_UINT8
        dims: [32, 100, 3]
        random_data: true
      }
    }
  }
]

# ç‰ˆæœ¬ç­–ç•¥
version_policy: { latest { num_versions: 1 }}

# ä¼˜åŒ–é…ç½®
optimization {
  cuda {
    graphs: true
  }
}
"""
    return config

def create_model_py():
    """åˆ›å»ºTriton Pythonåç«¯æ¨¡å‹æ–‡ä»¶"""
    
    model_code = '''# -*- coding: utf-8 -*-
"""
Triton Python Backend OCR Model
"""

import json
import numpy as np
import triton_python_backend_utils as pb_utils
from ultrafast_ocr.core import UltraFastOCR
import cv2

class TritonPythonModel:
    """Triton Python Backend Model for OCR"""
    
    def initialize(self, args):
        """åˆå§‹åŒ–æ¨¡å‹ - åªæ‰§è¡Œä¸€æ¬¡"""
        
        # è§£ææ¨¡å‹é…ç½®
        self.model_config = json.loads(args['model_config'])
        
        # è·å–GPU ID
        self.device_id = int(args['model_instance_device_id'])
        
        print(f"[OCR Model] Initializing on GPU {self.device_id}")
        
        # è®¾ç½®GPU
        import os
        os.environ['CUDA_VISIBLE_DEVICES'] = str(self.device_id)
        
        # åˆå§‹åŒ–OCRæ¨¡å‹ - åªåˆå§‹åŒ–ä¸€æ¬¡ï¼
        print(f"[OCR Model] Loading UltraFastOCR model...")
        self.ocr_model = UltraFastOCR()
        
        # é¢„çƒ­æ¨¡å‹
        self._warmup()
        
        print(f"[OCR Model] âœ… Model loaded and warmed up on GPU {self.device_id}")
        print(f"[OCR Model] ğŸ’¾ Model will stay in memory until server shutdown")
    
    def _warmup(self):
        """é¢„çƒ­æ¨¡å‹"""
        print(f"[OCR Model] Warming up model...")
        
        # åˆ›å»ºé¢„çƒ­å›¾åƒ
        warmup_images = []
        for i in range(3):
            h, w = (32 + i*16, 100 + i*50)
            img = np.ones((h, w, 3), dtype=np.uint8) * 255
            warmup_images.append(img)
        
        # é¢„çƒ­æ¨ç†
        for img in warmup_images:
            for _ in range(2):
                _ = self.ocr_model.recognize_single_line(img)
        
        print(f"[OCR Model] âœ… Warmup completed")
    
    def execute(self, requests):
        """æ‰§è¡Œæ¨ç† - æ¯æ¬¡è¯·æ±‚éƒ½è°ƒç”¨"""
        
        responses = []
        
        for request in requests:
            # è·å–è¾“å…¥
            image = pb_utils.get_input_tensor_by_name(request, "image")
            image_np = image.as_numpy()
            
            # OCRè¯†åˆ«
            text = self.ocr_model.recognize_single_line(image_np)
            
            # åˆ›å»ºè¾“å‡º
            output_tensor = pb_utils.Tensor(
                "text",
                np.array([text], dtype=object)
            )
            
            # åˆ›å»ºå“åº”
            response = pb_utils.InferenceResponse(
                output_tensors=[output_tensor]
            )
            responses.append(response)
        
        return responses
    
    def finalize(self):
        """æ¸…ç†èµ„æº - æœåŠ¡å™¨å…³é—­æ—¶è°ƒç”¨"""
        print(f"[OCR Model] Releasing model from GPU {self.device_id}")
        del self.ocr_model
'''
    return model_code

def setup_triton_repository():
    """è®¾ç½®Tritonæ¨¡å‹ä»“åº“"""
    
    print("ğŸš€ è®¾ç½®Triton Server OCRæ¨¡å‹ä»“åº“")
    
    # åˆ›å»ºç›®å½•ç»“æ„
    base_dir = Path("model_repository")
    model_dir = base_dir / "ocr_model"
    version_dir = model_dir / "1"
    
    # åˆ›å»ºç›®å½•
    version_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. åˆ›å»ºé…ç½®æ–‡ä»¶
    config_path = model_dir / "config.pbtxt"
    with open(config_path, 'w') as f:
        f.write(create_triton_config())
    print(f"âœ… åˆ›å»ºé…ç½®æ–‡ä»¶: {config_path}")
    
    # 2. åˆ›å»ºæ¨¡å‹æ–‡ä»¶
    model_path = version_dir / "model.py"
    with open(model_path, 'w') as f:
        f.write(create_model_py())
    print(f"âœ… åˆ›å»ºæ¨¡å‹æ–‡ä»¶: {model_path}")
    
    print("\nğŸ“ æ¨¡å‹ä»“åº“ç»“æ„:")
    print("model_repository/")
    print("â””â”€â”€ ocr_model/")
    print("    â”œâ”€â”€ config.pbtxt")
    print("    â””â”€â”€ 1/")
    print("        â””â”€â”€ model.py")
    
    print("\nğŸ¯ ä¸‹ä¸€æ­¥:")
    print("1. å¯åŠ¨Triton Server:")
    print("   docker run --gpus all -p 8000:8000 -p 8001:8001 -p 8002:8002 \\")
    print("     -v ${PWD}/model_repository:/models \\")
    print("     nvcr.io/nvidia/tritonserver:23.10-py3 \\")
    print("     tritonserver --model-repository=/models")
    print("\n2. ä½¿ç”¨å®¢æˆ·ç«¯è°ƒç”¨:")
    print("   python triton_ocr_client.py")

if __name__ == "__main__":
    setup_triton_repository()
