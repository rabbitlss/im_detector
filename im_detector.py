# -*- coding: utf-8 -*-
"""
IMæ£€æµ‹å™¨ - å¿«é€Ÿæ¨ç†æ¨¡å—
"""

import os
import time
from typing import Dict, List, Optional, Tuple
import cv2
import numpy as np
from pathlib import Path

try:
    from ultralytics import YOLO
except ImportError:
    print("è¯·å®‰è£…ultralytics: pip install ultralytics")
    exit(1)

from config import CLASS_NAMES


class IMDetector:
    """IMç•Œé¢å…ƒç´ æ£€æµ‹å™¨"""
    
    def __init__(self, model_path: str = None, confidence: float = 0.5):
        """
        åˆå§‹åŒ–æ£€æµ‹å™¨
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            confidence: ç½®ä¿¡åº¦é˜ˆå€¼
        """
        self.classes = CLASS_NAMES
        self.confidence = confidence
        self.model = None
        
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
    
    def load_model(self, model_path: str) -> None:
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        try:
            self.model = YOLO(model_path)
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def predict(self, image_path: str, return_image: bool = False) -> Dict:
        """
        å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œæ£€æµ‹
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            return_image: æ˜¯å¦è¿”å›æ ‡æ³¨åçš„å›¾ç‰‡
            
        Returns:
            æ£€æµ‹ç»“æœå­—å…¸
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        # æ¨ç†
        start_time = time.time()
        results = self.model(image_path, conf=self.confidence)
        inference_time = time.time() - start_time
        
        # è§£æç»“æœ
        detections = self._parse_results(results)
        
        # æ·»åŠ å…ƒæ•°æ®
        detections['metadata'] = {
            'inference_time_ms': inference_time * 1000,
            'fps': 1.0 / inference_time,
            'image_path': image_path,
            'confidence_threshold': self.confidence
        }
        
        # å¯é€‰: è¿”å›æ ‡æ³¨å›¾ç‰‡
        if return_image:
            annotated_image = self._annotate_image(image_path, detections)
            detections['annotated_image'] = annotated_image
        
        return detections
    
    def predict_batch(self, image_paths: List[str]) -> List[Dict]:
        """
        æ‰¹é‡æ£€æµ‹
        
        Args:
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            
        Returns:
            æ£€æµ‹ç»“æœåˆ—è¡¨
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        results = []
        
        for image_path in image_paths:
            try:
                result = self.predict(image_path)
                results.append(result)
            except Exception as e:
                print(f"æ£€æµ‹å¤±è´¥ {image_path}: {e}")
                results.append({'error': str(e), 'image_path': image_path})
        
        return results
    
    def _parse_results(self, results) -> Dict:
        """è§£æYOLOæ£€æµ‹ç»“æœ"""
        detections = {}
        
        for r in results:
            boxes = r.boxes
            if boxes is None:
                continue
                
            for box in boxes:
                class_id = int(box.cls)
                if class_id >= len(self.classes):
                    continue
                    
                class_name = self.classes[class_id]
                confidence = float(box.conf)
                bbox = box.xyxy[0].tolist()  # [x1, y1, x2, y2]
                
                if class_name not in detections:
                    detections[class_name] = []
                
                detections[class_name].append({
                    'bbox': bbox,
                    'confidence': confidence,
                    'center': [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2],
                    'area': (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])
                })
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        for class_name in detections:
            detections[class_name].sort(key=lambda x: x['confidence'], reverse=True)
        
        return detections
    
    def _annotate_image(self, image_path: str, detections: Dict) -> np.ndarray:
        """åœ¨å›¾ç‰‡ä¸Šæ ‡æ³¨æ£€æµ‹ç»“æœ"""
        image = cv2.imread(image_path)
        if image is None:
            return None
        
        # é¢œè‰²é…ç½® (BGRæ ¼å¼)
        colors = {
            'receiver_avatar': (0, 255, 0),    # ç»¿è‰²
            'receiver_name': (255, 0, 0),      # è“è‰²
            'input_box': (0, 0, 255),          # çº¢è‰²
            'send_button': (255, 255, 0),      # é’è‰²
            'chat_message': (255, 0, 255),     # ç´«è‰²
            'contact_item': (0, 255, 255),     # é»„è‰²
            'user_avatar': (128, 0, 128)       # æ·±ç´«è‰²
        }
        
        for class_name, objects in detections.items():
            if class_name == 'metadata':
                continue
                
            color = colors.get(class_name, (128, 128, 128))
            
            for obj in objects:
                bbox = obj['bbox']
                confidence = obj['confidence']
                
                # ç”»è¾¹ç•Œæ¡†
                x1, y1, x2, y2 = map(int, bbox)
                cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)
                
                # ç”»æ ‡ç­¾
                label = f"{class_name}: {confidence:.2f}"
                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
                cv2.rectangle(image, (x1, y1 - label_size[1] - 10), 
                            (x1 + label_size[0], y1), color, -1)
                cv2.putText(image, label, (x1, y1 - 5), 
                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return image
    
    def extract_im_info(self, image_path: str) -> Dict:
        """
        æå–IMç•Œé¢çš„å…³é”®ä¿¡æ¯
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            
        Returns:
            æå–çš„ä¿¡æ¯å­—å…¸
        """
        detections = self.predict(image_path)
        
        info = {
            'receiver': None,
            'input_text': None,
            'contacts': [],
            'messages': [],
            'ui_elements': detections
        }
        
        # æå–æ¥æ”¶è€…ä¿¡æ¯
        if 'receiver_name' in detections and detections['receiver_name']:
            # é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„
            best_receiver = detections['receiver_name'][0]
            info['receiver'] = {
                'name_bbox': best_receiver['bbox'],
                'confidence': best_receiver['confidence']
            }
            
            # å°è¯•å…³è”å¤´åƒ
            if 'receiver_avatar' in detections:
                receiver_avatar = self._find_nearest_avatar(
                    best_receiver, detections['receiver_avatar']
                )
                if receiver_avatar:
                    info['receiver']['avatar_bbox'] = receiver_avatar['bbox']
        
        # æå–è¾“å…¥æ¡†ä¿¡æ¯
        if 'input_box' in detections and detections['input_box']:
            info['input_text'] = detections['input_box'][0]['bbox']
        
        # æå–è”ç³»äººåˆ—è¡¨
        if 'contact_item' in detections:
            info['contacts'] = [
                {
                    'bbox': item['bbox'], 
                    'confidence': item['confidence']
                }
                for item in detections['contact_item'][:10]  # æœ€å¤š10ä¸ªè”ç³»äºº
            ]
        
        # æå–èŠå¤©æ¶ˆæ¯
        if 'chat_message' in detections:
            info['messages'] = [
                {
                    'bbox': msg['bbox'],
                    'confidence': msg['confidence']
                }
                for msg in detections['chat_message']
            ]
        
        return info
    
    def _find_nearest_avatar(self, name_obj: Dict, avatars: List[Dict]) -> Optional[Dict]:
        """æ‰¾åˆ°æœ€è¿‘çš„å¤´åƒ"""
        if not avatars:
            return None
        
        name_center = name_obj['center']
        min_distance = float('inf')
        nearest_avatar = None
        
        for avatar in avatars:
            avatar_center = avatar['center']
            distance = np.sqrt(
                (name_center[0] - avatar_center[0]) ** 2 + 
                (name_center[1] - avatar_center[1]) ** 2
            )
            
            if distance < min_distance:
                min_distance = distance
                nearest_avatar = avatar
        
        # åªæœ‰è·ç¦»åˆç†æ‰è¿”å›ï¼ˆä¸è¶…è¿‡200åƒç´ ï¼‰
        if min_distance < 200:
            return nearest_avatar
        
        return None
    
    def benchmark(self, test_images_folder: str, num_images: int = 10) -> Dict:
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            test_images_folder: æµ‹è¯•å›¾ç‰‡æ–‡ä»¶å¤¹
            num_images: æµ‹è¯•å›¾ç‰‡æ•°é‡
            
        Returns:
            æ€§èƒ½æŒ‡æ ‡
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨load_model()")
        
        # è·å–æµ‹è¯•å›¾ç‰‡
        test_images = list(Path(test_images_folder).glob("*.jpg"))[:num_images]
        
        if len(test_images) == 0:
            raise ValueError(f"åœ¨ {test_images_folder} ä¸­æœªæ‰¾åˆ°æµ‹è¯•å›¾ç‰‡")
        
        print(f"ğŸ”¥ å¼€å§‹æ€§èƒ½æµ‹è¯•ï¼Œå…± {len(test_images)} å¼ å›¾ç‰‡...")
        
        inference_times = []
        detection_counts = []
        
        for i, img_path in enumerate(test_images):
            start_time = time.time()
            detections = self.predict(str(img_path))
            inference_time = time.time() - start_time
            
            inference_times.append(inference_time)
            
            # ç»Ÿè®¡æ£€æµ‹åˆ°çš„ç›®æ ‡æ•°é‡
            total_detections = sum(
                len(objects) for class_name, objects in detections.items()
                if class_name != 'metadata'
            )
            detection_counts.append(total_detections)
            
            if (i + 1) % 5 == 0:
                print(f"  å·²å¤„ç†: {i + 1}/{len(test_images)}")
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        avg_inference_time = np.mean(inference_times)
        std_inference_time = np.std(inference_times)
        min_inference_time = np.min(inference_times)
        max_inference_time = np.max(inference_times)
        
        metrics = {
            'num_images': len(test_images),
            'avg_inference_time_ms': avg_inference_time * 1000,
            'std_inference_time_ms': std_inference_time * 1000,
            'min_inference_time_ms': min_inference_time * 1000,
            'max_inference_time_ms': max_inference_time * 1000,
            'avg_fps': 1.0 / avg_inference_time,
            'min_fps': 1.0 / max_inference_time,
            'max_fps': 1.0 / min_inference_time,
            'avg_detections_per_image': np.mean(detection_counts),
            'total_detections': sum(detection_counts)
        }
        
        print("\nğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"  å¹³å‡æ¨ç†æ—¶é—´: {metrics['avg_inference_time_ms']:.2f} ms")
        print(f"  å¹³å‡FPS: {metrics['avg_fps']:.2f}")
        print(f"  å¹³å‡æ£€æµ‹æ•°é‡/å›¾: {metrics['avg_detections_per_image']:.1f}")
        
        return metrics


def main():
    """æµ‹è¯•æ£€æµ‹åŠŸèƒ½"""
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    model_path = "./models/best_im_detector.pt"
    
    if not os.path.exists(model_path):
        print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒç¨‹åºç”Ÿæˆæ¨¡å‹")
        return
    
    detector = IMDetector(model_path, confidence=0.5)
    
    # æµ‹è¯•å•å¼ å›¾ç‰‡æ£€æµ‹
    test_image = "./data/test_image.jpg"
    if os.path.exists(test_image):
        print("ğŸ” æµ‹è¯•å•å¼ å›¾ç‰‡æ£€æµ‹...")
        result = detector.predict(test_image, return_image=True)
        
        print("æ£€æµ‹ç»“æœ:")
        for class_name, objects in result.items():
            if class_name != 'metadata' and class_name != 'annotated_image':
                print(f"  {class_name}: {len(objects)} ä¸ª")
        
        # ä¿å­˜æ ‡æ³¨å›¾ç‰‡
        if 'annotated_image' in result and result['annotated_image'] is not None:
            cv2.imwrite("./results/annotated_result.jpg", result['annotated_image'])
            print("æ ‡æ³¨å›¾ç‰‡å·²ä¿å­˜: ./results/annotated_result.jpg")
        
        # æå–å…³é”®ä¿¡æ¯
        im_info = detector.extract_im_info(test_image)
        print("\næå–çš„IMä¿¡æ¯:")
        print(f"  æ¥æ”¶è€…: {'æœ‰' if im_info['receiver'] else 'æ— '}")
        print(f"  è¾“å…¥æ¡†: {'æœ‰' if im_info['input_text'] else 'æ— '}")
        print(f"  è”ç³»äººæ•°é‡: {len(im_info['contacts'])}")
        print(f"  æ¶ˆæ¯æ•°é‡: {len(im_info['messages'])}")
    
    # æ€§èƒ½æµ‹è¯•
    test_folder = "./data/test_images"
    if os.path.exists(test_folder):
        print("\nğŸš€ æ€§èƒ½æµ‹è¯•...")
        metrics = detector.benchmark(test_folder, num_images=5)


if __name__ == "__main__":
    main()
