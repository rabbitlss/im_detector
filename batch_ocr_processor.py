# -*- coding: utf-8 -*-
"""
æ‰¹é‡OCRè¯†åˆ«å¤„ç†å™¨
å®ç°ï¼šæ£€æµ‹å¤šä¸ªUIå…ƒç´ åï¼Œåªè°ƒç”¨ä¸€æ¬¡OCRè¯†åˆ«æ¨¡å‹è·å–æ‰€æœ‰æ–‡å­—ç»“æœ
"""

import cv2
import numpy as np
import time
from typing import List, Dict, Tuple, Optional
import os
from dataclasses import dataclass

@dataclass 
class TextRegion:
    """æ–‡å­—åŒºåŸŸä¿¡æ¯"""
    region_id: str
    bbox: Tuple[int, int, int, int]  # (x1, y1, x2, y2)
    image_crop: np.ndarray
    source_type: str  # 'chat_area', 'input_area', etc.
    confidence: float = 0.0

@dataclass
class BatchOCRResult:
    """æ‰¹é‡OCRç»“æœ"""
    region_id: str
    text_content: str
    confidence: float
    processing_time_ms: float

class BatchOCRProcessor:
    """æ‰¹é‡OCRè¯†åˆ«å¤„ç†å™¨"""
    
    def __init__(self, max_batch_size: int = 32, 
                 target_height: int = 48,  # å¢åŠ åˆ°48pxä»¥ä¿ç•™æ›´å¤šç»†èŠ‚
                 padding: int = 5,
                 use_separator: bool = True):
        """
        åˆå§‹åŒ–æ‰¹é‡OCRå¤„ç†å™¨
        
        Args:
            max_batch_size: æœ€å¤§æ‰¹å¤„ç†å¤§å°
            target_height: æ ‡å‡†åŒ–é«˜åº¦
            padding: å›¾åƒé—´å¡«å……åƒç´ 
            use_separator: æ˜¯å¦ä½¿ç”¨åˆ†éš”ç¬¦
        """
        self.max_batch_size = max_batch_size
        self.target_height = target_height
        self.padding = padding
        self.use_separator = use_separator
        self.total_processing_time = 0
        self.total_regions_processed = 0
        
        # æ–‡å­—å¤§å°åˆ†ç±»é˜ˆå€¼
        self.size_thresholds = {
            'small': (0, 20),     # å°å­—ä½“
            'medium': (20, 35),   # ä¸­ç­‰å­—ä½“  
            'large': (35, 100)    # å¤§å­—ä½“
        }
        
        # åˆ›å»ºåˆ†éš”ç¬¦å›¾åƒï¼ˆé»‘è‰²ç«–çº¿ï¼‰
        if self.use_separator:
            self.separator = self._create_separator()
    
    def _create_separator(self) -> np.ndarray:
        """åˆ›å»ºåˆ†éš”ç¬¦å›¾åƒï¼ˆé»‘è‰²ç«–çº¿ï¼Œæ˜“äºOCRè¯†åˆ«ä¸º | ï¼‰"""
        # åˆ›å»ºç™½è‰²èƒŒæ™¯
        separator = np.ones((self.target_height, self.padding, 3), dtype=np.uint8) * 255
        # åœ¨ä¸­é—´ç”»é»‘è‰²ç«–çº¿
        mid_x = self.padding // 2
        separator[:, mid_x:mid_x+1, :] = 0  # é»‘è‰²ç«–çº¿
        return separator
        
    def collect_text_regions(self, image: np.ndarray, 
                           yolo_detections: List[Dict],
                           line_splitter) -> List[TextRegion]:
        """
        ä»YOLOæ£€æµ‹ç»“æœä¸­æ”¶é›†æ‰€æœ‰æ–‡å­—åŒºåŸŸ
        
        Args:
            image: åŸå§‹å›¾åƒ
            yolo_detections: YOLOæ£€æµ‹ç»“æœ
            line_splitter: æ–‡å­—è¡Œåˆ†å‰²å™¨
            
        Returns:
            æ–‡å­—åŒºåŸŸåˆ—è¡¨
        """
        
        print(f"ğŸ“¥ æ”¶é›†æ–‡å­—åŒºåŸŸä¸­...")
        text_regions = []
        
        for detection in yolo_detections:
            class_name = detection['class']
            bbox = detection['bbox']
            confidence = detection.get('confidence', 0.0)
            
            # è£å‰ªæ£€æµ‹åŒºåŸŸ
            x1, y1, x2, y2 = bbox
            region = image[y1:y2, x1:x2]
            
            if region.size == 0:
                continue
                
            # ä½¿ç”¨è¡Œåˆ†å‰²å™¨è·å–æ–‡å­—è¡Œ
            lines = line_splitter._projection_method(region)
            
            # ä¸ºæ¯ä¸ªæ–‡å­—è¡Œåˆ›å»ºTextRegion
            for i, line in enumerate(lines):
                line_x1, line_y1, line_x2, line_y2 = line.bbox
                
                # è£å‰ªå•è¡Œæ–‡å­—å›¾åƒ
                line_crop = region[line_y1:line_y2, line_x1:line_x2]
                
                if line_crop.size == 0:
                    continue
                    
                region_id = f"{class_name}_{i+1}"
                
                text_region = TextRegion(
                    region_id=region_id,
                    bbox=(x1 + line_x1, y1 + line_y1, 
                          x1 + line_x2, y1 + line_y2),
                    image_crop=line_crop,
                    source_type=class_name,
                    confidence=line.confidence
                )
                
                text_regions.append(text_region)
                
        print(f"âœ… æ”¶é›†åˆ° {len(text_regions)} ä¸ªæ–‡å­—åŒºåŸŸ")
        return text_regions
    
    def create_batch_image(self, text_regions: List[TextRegion]) -> Tuple[np.ndarray, List[Dict]]:
        """
        å°†å¤šä¸ªæ–‡å­—åŒºåŸŸæ‹¼æ¥æˆæ‰¹å¤„ç†å›¾åƒ
        
        Args:
            text_regions: æ–‡å­—åŒºåŸŸåˆ—è¡¨
            
        Returns:
            æ‰¹å¤„ç†å›¾åƒå’ŒåŒºåŸŸæ˜ å°„ä¿¡æ¯
        """
        
        if not text_regions:
            return np.array([]), []
            
        print(f"ğŸ”§ åˆ›å»ºæ‰¹å¤„ç†å›¾åƒï¼ŒåŒ…å« {len(text_regions)} ä¸ªåŒºåŸŸ...")
        
        # æ ¹æ®åŸå§‹é«˜åº¦åˆ†ç»„ï¼Œé¿å…è¿‡åº¦ç¼©æ”¾
        height_groups = self._group_by_similar_height(text_regions)
        
        normalized_regions = []
        region_mappings = []
        
        current_x = 0
        
        for region in text_regions:
            # è°ƒæ•´å›¾åƒå¤§å°åˆ°ç›®æ ‡é«˜åº¦
            crop = region.image_crop
            h, w = crop.shape[:2]
            
            # æ™ºèƒ½ç¼©æ”¾ç­–ç•¥
            scale = self._calculate_optimal_scale(h)
            new_h = int(h * scale)
            new_w = int(w * scale)
            
            # è°ƒæ•´å¤§å°
            if len(crop.shape) == 3:
                resized = cv2.resize(crop, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
            else:
                resized = cv2.resize(crop, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
                resized = cv2.cvtColor(resized, cv2.COLOR_GRAY2BGR)
            
            # å¡«å……åˆ°æ ‡å‡†é«˜åº¦
            padded = self._pad_to_target_height(resized)
                
            normalized_regions.append(padded)
            
            # è®°å½•åŒºåŸŸåœ¨æ‰¹å¤„ç†å›¾åƒä¸­çš„ä½ç½®
            region_mapping = {
                'region_id': region.region_id,
                'source_type': region.source_type,
                'original_bbox': region.bbox,
                'batch_bbox': (current_x, 0, current_x + new_w, self.target_height),
                'confidence': region.confidence,
                'original_height': h,
                'scale_factor': scale
            }
            region_mappings.append(region_mapping)
            
            current_x += new_w + self.padding
            
        # æ‹¼æ¥æ‰€æœ‰åŒºåŸŸ
        if normalized_regions:
            # å¦‚æœä½¿ç”¨åˆ†éš”ç¬¦ï¼Œè°ƒæ•´æ€»å®½åº¦è®¡ç®—
            if self.use_separator:
                # æ¯ä¸ªåŒºåŸŸåé¢éƒ½æœ‰åˆ†éš”ç¬¦ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼‰
                total_width = sum(r.shape[1] for r in normalized_regions) + self.padding * (len(normalized_regions) - 1)
            else:
                total_width = current_x - self.padding  # å»æ‰æœ€åä¸€ä¸ªpadding
                
            batch_image = np.ones((self.target_height, total_width, 3), dtype=np.uint8) * 255
            
            current_x = 0
            for i, resized_region in enumerate(normalized_regions):
                h, w = resized_region.shape[:2]
                batch_image[0:h, current_x:current_x+w] = resized_region
                current_x += w
                
                # æ·»åŠ åˆ†éš”ç¬¦ï¼ˆé™¤äº†æœ€åä¸€ä¸ªåŒºåŸŸï¼‰
                if self.use_separator and i < len(normalized_regions) - 1:
                    batch_image[:, current_x:current_x+self.padding] = self.separator
                    current_x += self.padding
                elif not self.use_separator:
                    current_x += self.padding
                
            print(f"âœ… æ‰¹å¤„ç†å›¾åƒåˆ›å»ºå®Œæˆ: {batch_image.shape}")
            return batch_image, region_mappings
        else:
            return np.array([]), []
    
    def _calculate_optimal_scale(self, original_height: int) -> float:
        """
        è®¡ç®—æœ€ä¼˜ç¼©æ”¾æ¯”ä¾‹ï¼Œé¿å…è¿‡åº¦ç¼©æ”¾
        
        Args:
            original_height: åŸå§‹é«˜åº¦
            
        Returns:
            ç¼©æ”¾æ¯”ä¾‹
        """
        # æ ¹æ®åŸå§‹é«˜åº¦é€‰æ‹©ç¼©æ”¾ç­–ç•¥
        if original_height < 20:
            # å°å­—ä½“ï¼šæ”¾å¤§åˆ°ç›®æ ‡é«˜åº¦
            return self.target_height / original_height
        elif original_height < 35:
            # ä¸­ç­‰å­—ä½“ï¼šé€‚åº¦ç¼©æ”¾
            return min(self.target_height / original_height, 1.5)
        else:
            # å¤§å­—ä½“ï¼šé™åˆ¶ç¼©æ”¾æ¯”ä¾‹
            return min(self.target_height / original_height, 1.0)
    
    def _pad_to_target_height(self, image: np.ndarray) -> np.ndarray:
        """
        å°†å›¾åƒå¡«å……åˆ°ç›®æ ‡é«˜åº¦
        
        Args:
            image: è¾“å…¥å›¾åƒ
            
        Returns:
            å¡«å……åçš„å›¾åƒ
        """
        h, w = image.shape[:2]
        
        if h >= self.target_height:
            # å¦‚æœé«˜åº¦è¶…è¿‡ç›®æ ‡ï¼Œè£å‰ªä¸­å¿ƒéƒ¨åˆ†
            start_y = (h - self.target_height) // 2
            return image[start_y:start_y + self.target_height, :]
        
        # å¡«å……åˆ°ç›®æ ‡é«˜åº¦
        pad_top = (self.target_height - h) // 2
        pad_bottom = self.target_height - h - pad_top
        
        padded = np.ones((self.target_height, w, 3), dtype=np.uint8) * 255
        padded[pad_top:pad_top + h, :] = image
        
        return padded
    
    def _group_by_similar_height(self, text_regions: List[TextRegion]) -> Dict[str, List[TextRegion]]:
        """
        æ ¹æ®ç›¸ä¼¼é«˜åº¦åˆ†ç»„æ–‡å­—åŒºåŸŸ
        
        Args:
            text_regions: æ–‡å­—åŒºåŸŸåˆ—è¡¨
            
        Returns:
            æŒ‰é«˜åº¦åˆ†ç»„çš„å­—å…¸
        """
        groups = {'small': [], 'medium': [], 'large': []}
        
        for region in text_regions:
            h = region.image_crop.shape[0]
            
            for size_type, (min_h, max_h) in self.size_thresholds.items():
                if min_h <= h < max_h:
                    groups[size_type].append(region)
                    break
                    
        return groups
    
    def batch_ocr_recognition(self, batch_image: np.ndarray, 
                               region_mappings: List[Dict]) -> List[str]:
        """
        çœŸå®OCRè¯†åˆ«è¿‡ç¨‹ï¼ˆå•æ¬¡è°ƒç”¨å¤„ç†æ•´ä¸ªæ‰¹æ¬¡ï¼‰
        
        Args:
            batch_image: æ‰¹å¤„ç†å›¾åƒ  
            region_mappings: åŒºåŸŸæ˜ å°„ä¿¡æ¯
            
        Returns:
            å„ä¸ªåŒºåŸŸçš„è¯†åˆ«æ–‡æœ¬åˆ—è¡¨
        """
        
        if batch_image.size == 0:
            return []
            
        print(f"ğŸ” æ‰§è¡Œæ‰¹é‡OCRè¯†åˆ«...")
        start_time = time.time()
        
        # çœŸå®OCRæ¨¡å‹è°ƒç”¨
        from ultrafast_ocr.core import UltraFastOCR
        ocr = UltraFastOCR()
        
        # å…³é”®ï¼šåªè°ƒç”¨ä¸€æ¬¡OCRè¯†åˆ«æ•´ä¸ªæ‹¼æ¥åçš„å›¾åƒ
        combined_text = ocr.recognize_single_line(batch_image)
        
        # è®¡ç®—æ€»å¤„ç†æ—¶é—´
        total_time_ms = (time.time() - start_time) * 1000
        
        print(f"âœ… æ‰¹é‡OCRè¯†åˆ«å®Œæˆï¼ˆå•æ¬¡è°ƒç”¨ï¼‰:")
        print(f"   - OCRè°ƒç”¨æ¬¡æ•°: 1 æ¬¡")
        print(f"   - å¤„ç†åŒºåŸŸæ•°: {len(region_mappings)}")
        print(f"   - æ€»è€—æ—¶: {total_time_ms:.1f}ms")
        print(f"   - è¯†åˆ«ç»“æœ: {combined_text}")
        
        # åˆ†å‰²è¯†åˆ«ç»“æœåˆ°å„ä¸ªåŒºåŸŸï¼Œè¿”å›æ–‡æœ¬åˆ—è¡¨
        text_parts = self._split_combined_result(combined_text, len(region_mappings))
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.total_processing_time += total_time_ms
        self.total_regions_processed += len(region_mappings)
        
        # ç›´æ¥è¿”å›å„éƒ¨åˆ†æ–‡æœ¬çš„åˆ—è¡¨
        return text_parts
    
    def _split_combined_result(self, combined_text: str, num_regions: int) -> List[str]:
        """
        åˆ†å‰²åˆå¹¶çš„OCRç»“æœ
        
        Args:
            combined_text: åˆå¹¶çš„è¯†åˆ«æ–‡æœ¬
            num_regions: åŒºåŸŸæ•°é‡
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬åˆ—è¡¨
        """
        if not combined_text:
            return [''] * num_regions
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªåŒºåŸŸï¼Œç›´æ¥è¿”å›
        if num_regions == 1:
            return [combined_text]
        
        # å°è¯•å¤šç§åˆ†å‰²ç­–ç•¥
        # ç­–ç•¥1: æŒ‰ç«–çº¿åˆ†å‰²ï¼ˆä½¿ç”¨åˆ†éš”ç¬¦æ—¶ï¼ŒOCRåº”è¯¥è¯†åˆ«ä¸ºç«–çº¿ï¼‰
        if self.use_separator:
            # å°è¯•å¤šç§å¯èƒ½çš„ç«–çº¿å­—ç¬¦
            separators = ['|', 'ï½œ', 'I', 'l', '1']  # ç«–çº¿å¯èƒ½è¢«è¯†åˆ«ä¸ºè¿™äº›å­—ç¬¦
            for sep in separators:
                if sep in combined_text:
                    parts = combined_text.split(sep)
                    # å…è®¸ä¸€å®šçš„å®¹é”™ï¼ˆÂ±1ï¼‰
                    if abs(len(parts) - num_regions) <= 1:
                        # è°ƒæ•´åˆ°æ­£ç¡®æ•°é‡
                        if len(parts) > num_regions:
                            # åˆå¹¶æœ€åçš„éƒ¨åˆ†
                            parts = parts[:num_regions-1] + [sep.join(parts[num_regions-1:])]
                        elif len(parts) < num_regions:
                            # æ·»åŠ ç©ºå­—ç¬¦ä¸²
                            parts.extend([''] * (num_regions - len(parts)))
                        return [p.strip() for p in parts]
        
        # ç­–ç•¥2: æŒ‰å¤šä¸ªç©ºæ ¼åˆ†å‰²ï¼ˆä¸ä½¿ç”¨åˆ†éš”ç¬¦æ—¶ï¼‰
        import re
        parts = re.split(r'\s{2,}', combined_text)
        if abs(len(parts) - num_regions) <= 1:
            if len(parts) > num_regions:
                parts = parts[:num_regions]
            elif len(parts) < num_regions:
                parts.extend([''] * (num_regions - len(parts)))
            return [p.strip() for p in parts]
        
        # ç­–ç•¥3: æŒ‰å›ºå®šé•¿åº¦åˆ†å‰²ï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
        avg_len = len(combined_text) // num_regions
        parts = []
        for i in range(num_regions):
            start = i * avg_len
            end = start + avg_len if i < num_regions - 1 else len(combined_text)
            parts.append(combined_text[start:end].strip())
        return parts
    
    def simulate_ocr_recognition(self, batch_image: np.ndarray, 
                               region_mappings: List[Dict]) -> List[BatchOCRResult]:
        """
        æ¨¡æ‹ŸOCRè¯†åˆ«ï¼ˆç”¨äºæµ‹è¯•ï¼Œä¸éœ€è¦çœŸå®OCRæ¨¡å‹ï¼‰
        """
        # è°ƒç”¨çœŸå®OCRè·å–æ–‡æœ¬åˆ—è¡¨
        text_results = self.batch_ocr_recognition(batch_image, region_mappings)
        
        # è½¬æ¢ä¸ºBatchOCRResultå¯¹è±¡åˆ—è¡¨
        results = []
        for text, mapping in zip(text_results, region_mappings):
            result = BatchOCRResult(
                region_id=mapping['region_id'],
                text_content=text,
                confidence=0.95,
                processing_time_ms=0
            )
            results.append(result)
        
        return results
    
    def process_im_image_batch(self, image: np.ndarray,
                             yolo_detections: List[Dict],
                             line_splitter) -> Dict:
        """
        æ‰¹é‡å¤„ç†IMå›¾åƒä¸­çš„æ‰€æœ‰æ–‡å­—åŒºåŸŸ
        
        Args:
            image: IMæˆªå›¾
            yolo_detections: YOLOæ£€æµ‹ç»“æœ
            line_splitter: æ–‡å­—è¡Œåˆ†å‰²å™¨
            
        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡OCRå¤„ç†...")
        print(f"   è¾“å…¥: {len(yolo_detections)} ä¸ªYOLOæ£€æµ‹åŒºåŸŸ")
        
        start_time = time.time()
        
        # æ­¥éª¤1: æ”¶é›†æ‰€æœ‰æ–‡å­—åŒºåŸŸ
        text_regions = self.collect_text_regions(image, yolo_detections, line_splitter)
        
        if not text_regions:
            print("âŒ æœªå‘ç°ä»»ä½•æ–‡å­—åŒºåŸŸ")
            return {
                'success': False,
                'message': 'æœªå‘ç°æ–‡å­—åŒºåŸŸ',
                'results': [],
                'stats': {}
            }
        
        # æ­¥éª¤2: åˆ›å»ºæ‰¹å¤„ç†å›¾åƒ
        batch_image, region_mappings = self.create_batch_image(text_regions)
        
        # æ­¥éª¤3: æ‰§è¡Œæ‰¹é‡OCRè¯†åˆ«ï¼ˆæ ¸å¿ƒï¼šåªè°ƒç”¨ä¸€æ¬¡ï¼‰
        text_results = self.batch_ocr_recognition(batch_image, region_mappings)
        
        # å°†æ–‡æœ¬ç»“æœè½¬æ¢ä¸ºBatchOCRResultå¯¹è±¡
        ocr_results = []
        for i, (text, mapping) in enumerate(zip(text_results, region_mappings)):
            result = BatchOCRResult(
                region_id=mapping['region_id'],
                text_content=text,
                confidence=0.95,  # é»˜è®¤ç½®ä¿¡åº¦
                processing_time_ms=0  # ç¨åè®¡ç®—
            )
            ocr_results.append(result)
        
        # æ­¥éª¤4: æ•´ç†ç»“æœ
        total_time = (time.time() - start_time) * 1000
        
        # æŒ‰æºç±»å‹åˆ†ç»„ç»“æœ
        grouped_results = {}
        for result in ocr_results:
            source_type = None
            for mapping in region_mappings:
                if mapping['region_id'] == result.region_id:
                    source_type = mapping['source_type']
                    break
                    
            if source_type not in grouped_results:
                grouped_results[source_type] = []
                
            grouped_results[source_type].append({
                'region_id': result.region_id,
                'text': result.text_content,
                'confidence': result.confidence,
                'bbox': next(m['original_bbox'] for m in region_mappings 
                           if m['region_id'] == result.region_id)
            })
        
        # æ€§èƒ½ç»Ÿè®¡
        stats = {
            'total_regions': len(text_regions),
            'total_time_ms': total_time,
            'ocr_calls': 1,  # å…³é”®æŒ‡æ ‡ï¼šåªè°ƒç”¨1æ¬¡OCR
            'avg_time_per_region': total_time / len(text_regions),
            'regions_per_second': len(text_regions) / (total_time / 1000),
            'batch_image_size': batch_image.shape if batch_image.size > 0 else None
        }
        
        print(f"\nğŸ¯ æ‰¹é‡å¤„ç†å®Œæˆ:")
        print(f"   âœ… OCRè°ƒç”¨æ¬¡æ•°: {stats['ocr_calls']} æ¬¡ï¼ˆå…³é”®ä¼˜åŠ¿ï¼ï¼‰")
        print(f"   âœ… å¤„ç†åŒºåŸŸæ•°: {stats['total_regions']}")
        print(f"   âœ… æ€»è€—æ—¶: {stats['total_time_ms']:.1f}ms")  
        print(f"   âœ… å¤„ç†é€Ÿåº¦: {stats['regions_per_second']:.1f} åŒºåŸŸ/ç§’")
        
        return {
            'success': True,
            'results': grouped_results,
            'stats': stats,
            'batch_image': batch_image,
            'region_mappings': region_mappings
        }
        
    def save_batch_visualization(self, batch_result: Dict, output_dir: str = "batch_ocr_results"):
        """ä¿å­˜æ‰¹å¤„ç†å¯è§†åŒ–ç»“æœ"""
        
        os.makedirs(output_dir, exist_ok=True)
        
        if not batch_result['success']:
            return
            
        batch_image = batch_result['batch_image'] 
        region_mappings = batch_result['region_mappings']
        results = batch_result['results']
        
        # ä¿å­˜æ‰¹å¤„ç†å›¾åƒ
        if batch_image.size > 0:
            cv2.imwrite(f"{output_dir}/batch_image.jpg", batch_image)
            
            # åœ¨æ‰¹å¤„ç†å›¾åƒä¸Šæ ‡æ³¨åŒºåŸŸä¿¡æ¯
            annotated_batch = batch_image.copy()
            
            for mapping in region_mappings:
                bbox = mapping['batch_bbox']
                x1, y1, x2, y2 = bbox
                
                # ç»˜åˆ¶åŒºåŸŸè¾¹ç•Œ
                cv2.rectangle(annotated_batch, (x1, y1), (x2, y2), (0, 255, 0), 2)
                
                # æ ‡æ³¨åŒºåŸŸID
                cv2.putText(annotated_batch, mapping['region_id'], 
                          (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)
            
            cv2.imwrite(f"{output_dir}/annotated_batch.jpg", annotated_batch)
            
        # ä¿å­˜æ–‡å­—è¯†åˆ«ç»“æœ
        with open(f"{output_dir}/batch_ocr_results.txt", "w", encoding="utf-8") as f:
            f.write("æ‰¹é‡OCRè¯†åˆ«ç»“æœ\n")
            f.write("="*50 + "\n\n")
            
            stats = batch_result['stats']
            f.write("æ€§èƒ½ç»Ÿè®¡:\n")
            f.write(f"- OCRè°ƒç”¨æ¬¡æ•°: {stats['ocr_calls']} æ¬¡\n")
            f.write(f"- å¤„ç†åŒºåŸŸæ•°: {stats['total_regions']}\n")
            f.write(f"- æ€»è€—æ—¶: {stats['total_time_ms']:.1f}ms\n")
            f.write(f"- å¹³å‡è€—æ—¶: {stats['avg_time_per_region']:.1f}ms/åŒºåŸŸ\n")
            f.write(f"- å¤„ç†é€Ÿåº¦: {stats['regions_per_second']:.1f} åŒºåŸŸ/ç§’\n\n")
            
            f.write("è¯†åˆ«ç»“æœ:\n")
            f.write("-" * 30 + "\n")
            
            for source_type, regions in results.items():
                f.write(f"\n{source_type}:\n")
                for region in regions:
                    f.write(f"  {region['region_id']}: {region['text']} "
                           f"(ç½®ä¿¡åº¦:{region['confidence']:.3f})\n")
        
        print(f"ğŸ’¾ æ‰¹å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {output_dir}/")
        
    def get_performance_summary(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡æ‘˜è¦"""
        
        if self.total_regions_processed == 0:
            return {'message': 'å°šæœªå¤„ç†ä»»ä½•åŒºåŸŸ'}
            
        return {
            'total_regions_processed': self.total_regions_processed,
            'total_processing_time_ms': self.total_processing_time,
            'average_time_per_region': self.total_processing_time / self.total_regions_processed,
            'estimated_speedup_vs_individual': self.total_regions_processed,  # ç†è®ºåŠ é€Ÿå€æ•°
            'regions_per_second': self.total_regions_processed / (self.total_processing_time / 1000)
        }


def demo_batch_ocr_processing():
    """æ¼”ç¤ºæ‰¹é‡OCRå¤„ç†æ•ˆæœ"""
    
    print("ğŸ­ æ‰¹é‡OCRå¤„ç†æ¼”ç¤º")
    print("="*60)
    
    # å¯¼å…¥ä¾èµ–
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent))
    from fast_text_line_splitter import ProjectionLineSplitter
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    batch_processor = BatchOCRProcessor(max_batch_size=32, target_height=32)
    line_splitter = ProjectionLineSplitter()
    
    # æ¨¡æ‹ŸIMå›¾åƒå’ŒYOLOæ£€æµ‹ç»“æœ
    # è¿™é‡Œå¯ä»¥ä½¿ç”¨çœŸå®çš„WeChatæˆªå›¾
    test_image = np.ones((600, 800, 3), dtype=np.uint8) * 255
    
    # æ¨¡æ‹Ÿå¤šä¸ªæ£€æµ‹åŒºåŸŸ
    mock_detections = [
        {
            'class': 'chat_area',
            'bbox': [50, 100, 750, 300],
            'confidence': 0.95
        },
        {
            'class': 'input_area', 
            'bbox': [50, 350, 750, 450],
            'confidence': 0.90
        },
        {
            'class': 'side_info',
            'bbox': [50, 470, 750, 550],
            'confidence': 0.85
        }
    ]
    
    # åœ¨æµ‹è¯•å›¾åƒä¸Šç»˜åˆ¶ä¸€äº›æ¨¡æ‹Ÿæ–‡å­—åŒºåŸŸ
    for detection in mock_detections:
        x1, y1, x2, y2 = detection['bbox']
        cv2.rectangle(test_image, (x1, y1), (x2, y2), (200, 200, 200), -1)
        
        # æ·»åŠ æ¨¡æ‹Ÿæ–‡å­—è¡Œ
        for i in range(3):  # æ¯ä¸ªåŒºåŸŸ3è¡Œæ–‡å­—
            line_y = y1 + 20 + i * 40
            cv2.rectangle(test_image, (x1+10, line_y), (x2-10, line_y+25), (100, 100, 100), -1)
    
    print(f"ğŸ“¸ æµ‹è¯•å›¾åƒå‡†å¤‡å®Œæˆ: {test_image.shape}")
    print(f"ğŸ¯ æ¨¡æ‹ŸYOLOæ£€æµ‹: {len(mock_detections)} ä¸ªåŒºåŸŸ")
    
    # æ‰§è¡Œæ‰¹é‡å¤„ç†
    batch_result = batch_processor.process_im_image_batch(
        test_image, mock_detections, line_splitter
    )
    
    # ä¿å­˜å¯è§†åŒ–ç»“æœ
    batch_processor.save_batch_visualization(batch_result)
    
    # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
    performance = batch_processor.get_performance_summary()
    print(f"\nğŸ“Š æ€»ä½“æ€§èƒ½ç»Ÿè®¡:")
    print(f"   å¤„ç†åŒºåŸŸæ€»æ•°: {performance['total_regions_processed']}")
    print(f"   æ€»è€—æ—¶: {performance['total_processing_time_ms']:.1f}ms")
    print(f"   å¹³å‡è€—æ—¶: {performance['average_time_per_region']:.1f}ms/åŒºåŸŸ")
    print(f"   ç›¸æ¯”å•ç‹¬è°ƒç”¨åŠ é€Ÿ: {performance['estimated_speedup_vs_individual']:.1f}x")
    print(f"   å¤„ç†é€Ÿåº¦: {performance['regions_per_second']:.1f} åŒºåŸŸ/ç§’")
    
    print(f"\nğŸ‰ æ‰¹é‡OCRå¤„ç†æ¼”ç¤ºå®Œæˆï¼")
    print(f"ğŸ’¡ æ ¸å¿ƒä¼˜åŠ¿ï¼š{batch_result['stats']['ocr_calls']}æ¬¡æ¨¡å‹è°ƒç”¨ vs {batch_result['stats']['total_regions']}æ¬¡å•ç‹¬è°ƒç”¨")

if __name__ == "__main__":
    demo_batch_ocr_processing()
