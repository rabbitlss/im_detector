# -*- coding: utf-8 -*-
"""
YOLOv11 è®­ç»ƒå™¨
ä½¿ç”¨æœ€æ–°çš„YOLOv11è¿›è¡ŒIMç•Œé¢å…ƒç´ æ£€æµ‹
"""

import os
import yaml
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import cv2
import numpy as np
from ultralytics import YOLO
import torch
from tqdm import tqdm
import json
from datetime import datetime


class YOLOv11Trainer:
    """YOLOv11æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, project_name: str = "im_detection"):
        """
        åˆå§‹åŒ–YOLOv11è®­ç»ƒå™¨
        
        Args:
            project_name: é¡¹ç›®åç§°
        """
        self.project_name = project_name
        self.model = None
        self.dataset_yaml = None
        
        # YOLOv11 æ¨¡å‹å˜ä½“
        self.model_variants = {
            'nano': 'yolov11n.pt',      # 5.4MB, æœ€å¿«
            'small': 'yolov11s.pt',     # 11.1MB, å¿«é€Ÿ
            'medium': 'yolov11m.pt',    # 38.8MB, å¹³è¡¡
            'large': 'yolov11l.pt',     # 65.9MB, å‡†ç¡®
            'xlarge': 'yolov11x.pt'     # 109.3MB, æœ€å‡†ç¡®
        }
        
        # ç±»åˆ«æ˜ å°„
        self.class_names = [
            'receiver_avatar',
            'receiver_name',
            'input_box',
            'send_button',
            'chat_message',
            'contact_item',
            'user_avatar'
        ]
        
    def prepare_dataset(self, labeled_data_path: str, 
                       train_ratio: float = 0.8,
                       val_ratio: float = 0.15,
                       test_ratio: float = 0.05) -> str:
        """
        å‡†å¤‡YOLOv11æ•°æ®é›†
        
        Args:
            labeled_data_path: æ ‡æ³¨æ•°æ®è·¯å¾„
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
        """
        print("ğŸ“Š å‡†å¤‡YOLOv11æ•°æ®é›†...")
        
        # åˆ›å»ºæ•°æ®é›†ç›®å½•ç»“æ„
        dataset_path = Path(f"./datasets/{self.project_name}")
        for split in ['train', 'val', 'test']:
            (dataset_path / 'images' / split).mkdir(parents=True, exist_ok=True)
            (dataset_path / 'labels' / split).mkdir(parents=True, exist_ok=True)
        
        # è·å–æ‰€æœ‰æ ‡æ³¨æ–‡ä»¶
        labeled_path = Path(labeled_data_path)
        image_files = list((labeled_path / 'images').glob('*.[jp][pn][g]'))
        
        # æ‰“ä¹±å¹¶åˆ†å‰²æ•°æ®é›†
        np.random.shuffle(image_files)
        n_total = len(image_files)
        n_train = int(n_total * train_ratio)
        n_val = int(n_total * val_ratio)
        
        splits = {
            'train': image_files[:n_train],
            'val': image_files[n_train:n_train+n_val],
            'test': image_files[n_train+n_val:]
        }
        
        # å¤åˆ¶æ–‡ä»¶åˆ°å¯¹åº”ç›®å½•
        for split, files in splits.items():
            print(f"  {split}: {len(files)} å¼ å›¾ç‰‡")
            for img_file in files:
                # å¤åˆ¶å›¾ç‰‡
                shutil.copy2(img_file, dataset_path / 'images' / split / img_file.name)
                
                # å¤åˆ¶æ ‡æ³¨
                label_file = labeled_path / 'labels' / f"{img_file.stem}.txt"
                if label_file.exists():
                    shutil.copy2(label_file, dataset_path / 'labels' / split / label_file.name)
        
        # åˆ›å»ºYOLOv11æ•°æ®é›†é…ç½®æ–‡ä»¶
        yaml_path = dataset_path / 'data.yaml'
        data_config = {
            'path': str(dataset_path.absolute()),
            'train': 'images/train',
            'val': 'images/val',
            'test': 'images/test',
            'nc': len(self.class_names),
            'names': self.class_names,
            
            # YOLOv11 ç‰¹å®šé…ç½®
            'download': None,
            
            # YOLOv11 æ•°æ®å¢å¼ºé…ç½®
            'augmentation': {
                'hsv_h': 0.015,    # HSVè‰²è°ƒ
                'hsv_s': 0.7,      # HSVé¥±å’Œåº¦
                'hsv_v': 0.4,      # HSVæ˜åº¦
                'degrees': 0.0,    # æ—‹è½¬è§’åº¦ï¼ˆIMç•Œé¢ä¸éœ€è¦æ—‹è½¬ï¼‰
                'translate': 0.1,  # å¹³ç§»
                'scale': 0.5,      # ç¼©æ”¾
                'shear': 0.0,      # å‰ªåˆ‡ï¼ˆIMç•Œé¢ä¸éœ€è¦ï¼‰
                'perspective': 0.0, # é€è§†å˜æ¢
                'flipud': 0.0,     # ä¸Šä¸‹ç¿»è½¬ï¼ˆIMç•Œé¢ä¸éœ€è¦ï¼‰
                'fliplr': 0.5,     # å·¦å³ç¿»è½¬
                'mosaic': 1.0,     # Mosaicå¢å¼º
                'mixup': 0.0,      # MixUpå¢å¼º
                'copy_paste': 0.0  # Copy-Pasteå¢å¼º
            }
        }
        
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(data_config, f, allow_unicode=True)
        
        self.dataset_yaml = str(yaml_path)
        print(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ: {yaml_path}")
        
        return str(yaml_path)
    
    def train_model(self, 
                   dataset_config: Optional[str] = None,
                   model_size: str = 'nano',
                   epochs: int = 100,
                   batch_size: int = 16,
                   img_size: int = 640,
                   device: str = 'auto',
                   patience: int = 50,
                   workers: int = 8,
                   resume: bool = False,
                   optimizer: str = 'AdamW',
                   lr0: float = 0.01,
                   amp: bool = True) -> YOLO:
        """
        è®­ç»ƒYOLOv11æ¨¡å‹
        
        Args:
            dataset_config: æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
            model_size: æ¨¡å‹å¤§å° (nano/small/medium/large/xlarge)
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            img_size: è¾“å…¥å›¾ç‰‡å°ºå¯¸
            device: è®¾å¤‡ ('auto', 'cpu', '0', '0,1'ç­‰)
            patience: æ—©åœè€å¿ƒå€¼
            workers: æ•°æ®åŠ è½½çº¿ç¨‹æ•°
            resume: æ˜¯å¦æ¢å¤è®­ç»ƒ
            optimizer: ä¼˜åŒ–å™¨ (SGD, Adam, AdamW, RMSProp)
            lr0: åˆå§‹å­¦ä¹ ç‡
            amp: æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
            
        Returns:
            è®­ç»ƒåçš„æ¨¡å‹
        """
        if dataset_config:
            self.dataset_yaml = dataset_config
        
        if not self.dataset_yaml:
            raise ValueError("è¯·å…ˆå‡†å¤‡æ•°æ®é›†æˆ–æä¾›dataset_config")
        
        print(f"ğŸš‚ å¼€å§‹è®­ç»ƒYOLOv11-{model_size}æ¨¡å‹...")
        print(f"  æ•°æ®é›†: {self.dataset_yaml}")
        print(f"  è®¾å¤‡: {device}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  è®­ç»ƒè½®æ•°: {epochs}")
        
        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        if device == 'auto':
            device = '0' if torch.cuda.is_available() else 'cpu'
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        model_path = self.model_variants.get(model_size, 'yolov11n.pt')
        self.model = YOLO(model_path)
        
        # YOLOv11 è®­ç»ƒå‚æ•°
        results = self.model.train(
            data=self.dataset_yaml,
            epochs=epochs,
            batch=batch_size,
            imgsz=img_size,
            device=device,
            workers=workers,
            patience=patience,
            
            # ä¼˜åŒ–å™¨è®¾ç½®
            optimizer=optimizer,
            lr0=lr0,               # åˆå§‹å­¦ä¹ ç‡
            lrf=0.01,             # æœ€ç»ˆå­¦ä¹ ç‡å› å­
            momentum=0.937,        # SGDåŠ¨é‡/Adam beta1
            weight_decay=0.0005,   # æƒé‡è¡°å‡
            warmup_epochs=3.0,     # é¢„çƒ­è½®æ•°
            warmup_momentum=0.8,   # é¢„çƒ­åŠ¨é‡
            warmup_bias_lr=0.1,    # é¢„çƒ­åç½®å­¦ä¹ ç‡
            
            # æŸå¤±å‡½æ•°æƒé‡ (YOLOv11ä¼˜åŒ–)
            box=7.5,              # è¾¹ç•Œæ¡†æŸå¤±æƒé‡
            cls=0.5,              # åˆ†ç±»æŸå¤±æƒé‡
            dfl=1.5,              # åˆ†å¸ƒå¼ç„¦ç‚¹æŸå¤±æƒé‡ (v11æ–°å¢)
            
            # æ•°æ®å¢å¼º (YOLOv11å¢å¼º)
            hsv_h=0.015,          # HSVè‰²è°ƒå¢å¼º
            hsv_s=0.7,            # HSVé¥±å’Œåº¦å¢å¼º
            hsv_v=0.4,            # HSVæ˜åº¦å¢å¼º
            degrees=0.0,          # æ—‹è½¬è§’åº¦
            translate=0.1,        # å¹³ç§»
            scale=0.5,            # ç¼©æ”¾
            shear=0.0,            # å‰ªåˆ‡
            perspective=0.0,      # é€è§†
            flipud=0.0,           # ä¸Šä¸‹ç¿»è½¬
            fliplr=0.5,           # å·¦å³ç¿»è½¬
            bgr=0.0,              # BGRé€šé“ç¿»è½¬æ¦‚ç‡
            mosaic=1.0,           # Mosaicå¢å¼º
            mixup=0.0,            # MixUpå¢å¼º
            copy_paste=0.0,       # Copy-Pasteå¢å¼º
            auto_augment='randaugment',  # è‡ªåŠ¨å¢å¼ºç­–ç•¥ (v11æ–°å¢)
            erasing=0.0,          # éšæœºæ“¦é™¤æ¦‚ç‡ (v11æ–°å¢)
            
            # è®­ç»ƒè®¾ç½®
            pretrained=True,      # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            resume=resume,        # æ¢å¤è®­ç»ƒ
            amp=amp,              # è‡ªåŠ¨æ··åˆç²¾åº¦
            fraction=1.0,         # æ•°æ®é›†ä½¿ç”¨æ¯”ä¾‹
            profile=False,        # æ€§èƒ½åˆ†æ
            freeze=None,          # å†»ç»“å±‚æ•°
            
            # å¤šGPUè®¾ç½®
            multi_scale=False,    # å¤šå°ºåº¦è®­ç»ƒ
            single_cls=False,     # å•ç±»åˆ«è®­ç»ƒ
            
            # NMSè®¾ç½®
            nms_time_limit=10.0,  # NMSæ—¶é—´é™åˆ¶
            
            # ä¿å­˜å’Œæ—¥å¿—
            save=True,            # ä¿å­˜æ£€æŸ¥ç‚¹
            save_period=-1,       # ä¿å­˜é—´éš”
            cache=False,          # ç¼“å­˜å›¾ç‰‡åˆ°å†…å­˜
            plots=True,           # ç»˜åˆ¶è®­ç»ƒå›¾è¡¨
            
            # éªŒè¯è®¾ç½®
            val_period=1,         # éªŒè¯é—´éš”
            sync_bn=False,        # åŒæ­¥æ‰¹å½’ä¸€åŒ–
            
            # æ—©åœè®¾ç½®
            close_mosaic=10,      # æœ€åNè½®å…³é—­Mosaic
            
            # é¡¹ç›®è®¾ç½®
            project=f'runs/{self.project_name}',
            name=f'train_{model_size}_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
            exist_ok=False,
            
            # å…¶ä»–
            seed=0,               # éšæœºç§å­
            deterministic=True,   # ç¡®å®šæ€§è®­ç»ƒ
            rect=False,           # çŸ©å½¢è®­ç»ƒ
            cos_lr=False,         # ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦
            overlap_mask=True,    # æ©ç é‡å  (v11)
            mask_ratio=4,         # æ©ç ä¸‹é‡‡æ ·æ¯”ä¾‹ (v11)
            dropout=0.0,          # Dropoutæ¦‚ç‡ (v11)
            verbose=True          # è¯¦ç»†è¾“å‡º
        )
        
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"  æœ€ä½³æ¨¡å‹: runs/{self.project_name}/train_{model_size}_*/weights/best.pt")
        
        return self.model
    
    def predict(self, 
               source,
               model_path: Optional[str] = None,
               conf: float = 0.25,
               iou: float = 0.45,
               img_size: int = 640,
               device: str = 'auto',
               save: bool = False,
               save_txt: bool = False,
               save_conf: bool = False,
               save_crop: bool = False,
               show: bool = False,
               stream: bool = False,
               verbose: bool = True,
               half: bool = False,
               max_det: int = 300,
               vid_stride: int = 1,
               line_width: Optional[int] = None,
               visualize: bool = False,
               augment: bool = False,
               agnostic_nms: bool = False,
               classes: Optional[List[int]] = None,
               retina_masks: bool = False) -> List:
        """
        ä½¿ç”¨YOLOv11è¿›è¡Œé¢„æµ‹
        
        Args:
            source: å›¾ç‰‡è·¯å¾„ã€è§†é¢‘è·¯å¾„ã€ç›®å½•è·¯å¾„ã€URLç­‰
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨self.modelï¼‰
            conf: ç½®ä¿¡åº¦é˜ˆå€¼
            iou: NMSçš„IoUé˜ˆå€¼
            img_size: æ¨ç†å›¾ç‰‡å°ºå¯¸
            device: è®¾å¤‡
            save: æ˜¯å¦ä¿å­˜é¢„æµ‹ç»“æœ
            save_txt: æ˜¯å¦ä¿å­˜æ–‡æœ¬ç»“æœ
            save_conf: æ˜¯å¦åœ¨æ–‡æœ¬ä¸­ä¿å­˜ç½®ä¿¡åº¦
            save_crop: æ˜¯å¦ä¿å­˜è£å‰ªçš„æ£€æµ‹æ¡†
            show: æ˜¯å¦æ˜¾ç¤ºç»“æœ
            stream: æ˜¯å¦ä½¿ç”¨æµå¼å¤„ç†ï¼ˆç”¨äºè§†é¢‘ï¼‰
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            half: æ˜¯å¦ä½¿ç”¨FP16æ¨ç†
            max_det: æ¯å¼ å›¾ç‰‡æœ€å¤§æ£€æµ‹æ•°
            vid_stride: è§†é¢‘å¸§æ­¥é•¿
            line_width: è¾¹ç•Œæ¡†çº¿å®½
            visualize: æ˜¯å¦å¯è§†åŒ–ç‰¹å¾å›¾
            augment: æ˜¯å¦ä½¿ç”¨æµ‹è¯•æ—¶å¢å¼º
            agnostic_nms: æ˜¯å¦ä½¿ç”¨ç±»åˆ«æ— å…³çš„NMS
            classes: åªæ£€æµ‹æŒ‡å®šç±»åˆ«
            retina_masks: æ˜¯å¦ä½¿ç”¨é«˜åˆ†è¾¨ç‡æ©ç 
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        # åŠ è½½æ¨¡å‹
        if model_path:
            model = YOLO(model_path)
        elif self.model:
            model = self.model
        else:
            raise ValueError("è¯·æä¾›æ¨¡å‹è·¯å¾„æˆ–å…ˆè®­ç»ƒæ¨¡å‹")
        
        # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        if device == 'auto':
            device = '0' if torch.cuda.is_available() else 'cpu'
        
        # YOLOv11 é¢„æµ‹
        results = model.predict(
            source=source,
            conf=conf,
            iou=iou,
            imgsz=img_size,
            device=device,
            save=save,
            save_txt=save_txt,
            save_conf=save_conf,
            save_crop=save_crop,
            show=show,
            stream=stream,
            verbose=verbose,
            half=half,
            max_det=max_det,
            vid_stride=vid_stride,
            line_width=line_width,
            visualize=visualize,
            augment=augment,
            agnostic_nms=agnostic_nms,
            classes=classes,
            retina_masks=retina_masks,
            
            # YOLOv11 ç‰¹å®šå‚æ•°
            embed=None,           # ç‰¹å¾åµŒå…¥å±‚
            project=f'runs/{self.project_name}',
            name='predict',
            exist_ok=True
        )
        
        return results
    
    def predict_single(self, image_path: str, 
                      model_path: Optional[str] = None,
                      return_format: str = 'dict') -> Dict:
        """
        é¢„æµ‹å•å¼ å›¾ç‰‡ï¼ˆç®€åŒ–æ¥å£ï¼‰
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            model_path: æ¨¡å‹è·¯å¾„
            return_format: è¿”å›æ ¼å¼ ('dict', 'json', 'yolo')
            
        Returns:
            æ£€æµ‹ç»“æœå­—å…¸
        """
        results = self.predict(
            source=image_path,
            model_path=model_path,
            verbose=False,
            stream=False
        )
        
        if not results:
            return {"objects": []}
        
        result = results[0]
        
        if return_format == 'yolo':
            return result
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        detections = []
        for box in result.boxes:
            x1, y1, x2, y2 = box.xyxy[0].tolist()
            conf = box.conf[0].item()
            cls = int(box.cls[0].item())
            
            detections.append({
                'class': self.class_names[cls],
                'class_id': cls,
                'bbox': [int(x1), int(y1), int(x2), int(y2)],
                'confidence': round(conf, 3)
            })
        
        output = {
            'image_path': image_path,
            'image_size': result.orig_shape,
            'objects': detections,
            'count': len(detections)
        }
        
        if return_format == 'json':
            return json.dumps(output, ensure_ascii=False, indent=2)
        
        return output
    
    def predict_batch(self, image_dir: str, 
                     model_path: Optional[str] = None,
                     save_results: bool = True) -> List[Dict]:
        """
        æ‰¹é‡é¢„æµ‹
        
        Args:
            image_dir: å›¾ç‰‡ç›®å½•
            model_path: æ¨¡å‹è·¯å¾„
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            
        Returns:
            æ‰€æœ‰é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        image_paths = list(Path(image_dir).glob('*.[jp][pn][g]'))
        
        all_results = []
        
        print(f"ğŸ” æ‰¹é‡é¢„æµ‹ {len(image_paths)} å¼ å›¾ç‰‡...")
        
        for img_path in tqdm(image_paths, desc="é¢„æµ‹è¿›åº¦"):
            result = self.predict_single(str(img_path), model_path)
            all_results.append(result)
        
        if save_results:
            output_file = Path(image_dir) / 'predictions.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
            print(f"âœ… ç»“æœå·²ä¿å­˜: {output_file}")
        
        # ç»Ÿè®¡
        total_objects = sum(r['count'] for r in all_results)
        avg_objects = total_objects / len(all_results) if all_results else 0
        
        print(f"\nğŸ“Š é¢„æµ‹ç»Ÿè®¡:")
        print(f"  æ€»å›¾ç‰‡æ•°: {len(all_results)}")
        print(f"  æ€»æ£€æµ‹æ•°: {total_objects}")
        print(f"  å¹³å‡æ¯å¼ : {avg_objects:.1f} ä¸ªå¯¹è±¡")
        
        return all_results
    
    def evaluate_model(self, model_path: Optional[str] = None) -> Dict:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        if model_path:
            model = YOLO(model_path)
        elif self.model:
            model = self.model
        else:
            raise ValueError("è¯·æä¾›æ¨¡å‹è·¯å¾„æˆ–å…ˆè®­ç»ƒæ¨¡å‹")
        
        print("ğŸ“ˆ è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        metrics = model.val(
            data=self.dataset_yaml,
            batch=16,
            imgsz=640,
            conf=0.001,
            iou=0.6,
            device='0' if torch.cuda.is_available() else 'cpu',
            
            # YOLOv11 è¯„ä¼°å‚æ•°
            plots=True,
            save_json=True,
            save_hybrid=False,
            max_det=300,
            half=True,
            dnn=False,
            verbose=True
        )
        
        # æå–å…³é”®æŒ‡æ ‡
        results = {
            'mAP50': float(metrics.box.map50),
            'mAP50-95': float(metrics.box.map),
            'precision': float(metrics.box.mp),
            'recall': float(metrics.box.mr),
            'f1_score': 2 * float(metrics.box.mp) * float(metrics.box.mr) / 
                       (float(metrics.box.mp) + float(metrics.box.mr) + 1e-6)
        }
        
        # æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½
        class_metrics = {}
        for i, cls_name in enumerate(self.class_names):
            class_metrics[cls_name] = {
                'AP50': float(metrics.box.ap50[i]) if i < len(metrics.box.ap50) else 0,
                'AP': float(metrics.box.ap[i]) if i < len(metrics.box.ap) else 0
            }
        
        results['per_class'] = class_metrics
        
        print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"  mAP@50: {results['mAP50']:.3f}")
        print(f"  mAP@50-95: {results['mAP50-95']:.3f}")
        print(f"  Precision: {results['precision']:.3f}")
        print(f"  Recall: {results['recall']:.3f}")
        print(f"  F1-Score: {results['f1_score']:.3f}")
        
        return results
    
    def export_model(self, model_path: Optional[str] = None,
                    formats: List[str] = ['onnx']) -> Dict[str, str]:
        """
        å¯¼å‡ºæ¨¡å‹ä¸ºå…¶ä»–æ ¼å¼
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            formats: å¯¼å‡ºæ ¼å¼åˆ—è¡¨ 
                    ['onnx', 'torchscript', 'coreml', 'tflite', 
                     'paddle', 'ncnn', 'engine']
                     
        Returns:
            å¯¼å‡ºæ–‡ä»¶è·¯å¾„å­—å…¸
        """
        if model_path:
            model = YOLO(model_path)
        elif self.model:
            model = self.model
        else:
            raise ValueError("è¯·æä¾›æ¨¡å‹è·¯å¾„æˆ–å…ˆè®­ç»ƒæ¨¡å‹")
        
        exported_paths = {}
        
        for fmt in formats:
            print(f"ğŸ“¦ å¯¼å‡ºä¸º {fmt} æ ¼å¼...")
            
            try:
                path = model.export(
                    format=fmt,
                    imgsz=640,
                    half=False,
                    dynamic=True if fmt in ['onnx', 'engine'] else False,
                    simplify=True if fmt == 'onnx' else False,
                    opset=17 if fmt == 'onnx' else None,
                    workspace=4 if fmt == 'engine' else None,
                    nms=False,
                    batch=1
                )
                exported_paths[fmt] = path
                print(f"  âœ… {fmt}: {path}")
                
            except Exception as e:
                print(f"  âŒ {fmt} å¯¼å‡ºå¤±è´¥: {e}")
        
        return exported_paths
    
    def benchmark(self, model_path: Optional[str] = None,
                 test_images: Optional[str] = None,
                 num_images: int = 100) -> Dict:
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            test_images: æµ‹è¯•å›¾ç‰‡ç›®å½•
            num_images: æµ‹è¯•å›¾ç‰‡æ•°é‡
            
        Returns:
            æ€§èƒ½æŒ‡æ ‡
        """
        import time
        
        if model_path:
            model = YOLO(model_path)
        elif self.model:
            model = self.model
        else:
            raise ValueError("è¯·æä¾›æ¨¡å‹è·¯å¾„æˆ–å…ˆè®­ç»ƒæ¨¡å‹")
        
        # è·å–æµ‹è¯•å›¾ç‰‡
        if test_images:
            img_paths = list(Path(test_images).glob('*.[jp][pn][g]'))[:num_images]
        else:
            # ä½¿ç”¨éªŒè¯é›†
            dataset_path = Path(self.dataset_yaml).parent
            img_paths = list((dataset_path / 'images' / 'val').glob('*.[jp][pn][g]'))[:num_images]
        
        if not img_paths:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•å›¾ç‰‡")
        
        print(f"âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯• ({len(img_paths)} å¼ å›¾ç‰‡)...")
        
        # é¢„çƒ­
        for _ in range(3):
            _ = model.predict(str(img_paths[0]), verbose=False)
        
        # æµ‹è¯•
        times = []
        for img_path in tqdm(img_paths, desc="åŸºå‡†æµ‹è¯•"):
            start = time.time()
            _ = model.predict(str(img_path), verbose=False)
            times.append(time.time() - start)
        
        # è®¡ç®—ç»Ÿè®¡
        times = np.array(times)
        
        results = {
            'num_images': len(img_paths),
            'avg_time': float(np.mean(times)),
            'std_time': float(np.std(times)),
            'min_time': float(np.min(times)),
            'max_time': float(np.max(times)),
            'fps': float(1.0 / np.mean(times)),
            'ms_per_image': float(np.mean(times) * 1000)
        }
        
        print(f"\nğŸ“Š æ€§èƒ½ç»“æœ:")
        print(f"  å¹³å‡è€—æ—¶: {results['ms_per_image']:.2f} ms")
        print(f"  FPS: {results['fps']:.1f}")
        print(f"  æœ€å¿«: {results['min_time']*1000:.2f} ms")
        print(f"  æœ€æ…¢: {results['max_time']*1000:.2f} ms")
        
        return results


def demo():
    """æ¼”ç¤ºYOLOv11è®­ç»ƒå’Œé¢„æµ‹"""
    
    trainer = YOLOv11Trainer("im_detection_v11")
    
    # 1. å‡†å¤‡æ•°æ®é›†
    dataset_yaml = trainer.prepare_dataset("./data/labeled_data")
    
    # 2. è®­ç»ƒæ¨¡å‹
    model = trainer.train_model(
        dataset_config=dataset_yaml,
        model_size='nano',    # ä½¿ç”¨nanoç‰ˆæœ¬å¿«é€Ÿè®­ç»ƒ
        epochs=50,            # æ¼”ç¤ºç”¨50è½®
        batch_size=16,
        device='auto',
        patience=20,
        amp=True             # ä½¿ç”¨æ··åˆç²¾åº¦åŠ é€Ÿ
    )
    
    # 3. é¢„æµ‹å•å¼ å›¾ç‰‡
    result = trainer.predict_single(
        "./test_images/test1.jpg",
        return_format='dict'
    )
    print(f"\næ£€æµ‹ç»“æœ: {result}")
    
    # 4. æ‰¹é‡é¢„æµ‹
    batch_results = trainer.predict_batch(
        "./test_images",
        save_results=True
    )
    
    # 5. è¯„ä¼°æ¨¡å‹
    metrics = trainer.evaluate_model()
    
    # 6. å¯¼å‡ºæ¨¡å‹
    exported = trainer.export_model(formats=['onnx'])
    
    # 7. æ€§èƒ½æµ‹è¯•
    benchmark = trainer.benchmark(num_images=50)
    
    print("\nğŸ‰ YOLOv11æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    demo()
