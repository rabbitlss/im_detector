#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Triton Server OCRå®¢æˆ·ç«¯
æ¼”ç¤ºå¦‚ä½•è°ƒç”¨éƒ¨ç½²åœ¨Triton Serverä¸Šçš„OCRæ¨¡å‹
"""

import numpy as np
import cv2
import time
from typing import List, Dict
import tritonclient.http as httpclient
import tritonclient.grpc as grpcclient

class TritonOCRClient:
    """Triton OCRå®¢æˆ·ç«¯"""
    
    def __init__(self, 
                 server_url: str = "localhost:8000",
                 model_name: str = "ocr_model",
                 use_grpc: bool = False):
        """
        åˆå§‹åŒ–Tritonå®¢æˆ·ç«¯
        
        Args:
            server_url: TritonæœåŠ¡å™¨åœ°å€
            model_name: æ¨¡å‹åç§°
            use_grpc: æ˜¯å¦ä½¿ç”¨gRPCï¼ˆæ›´å¿«ï¼‰
        """
        self.server_url = server_url
        self.model_name = model_name
        self.use_grpc = use_grpc
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        if use_grpc:
            self.client = grpcclient.InferenceServerClient(
                url=server_url.replace("8000", "8001")
            )
        else:
            self.client = httpclient.InferenceServerClient(url=server_url)
        
        print(f"ğŸ”Œ Triton OCRå®¢æˆ·ç«¯åˆå§‹åŒ–")
        print(f"   æœåŠ¡å™¨: {server_url}")
        print(f"   æ¨¡å‹: {model_name}")
        print(f"   åè®®: {'gRPC' if use_grpc else 'HTTP'}")
        
        # æ£€æŸ¥æœåŠ¡å™¨çŠ¶æ€
        self._check_server()
    
    def _check_server(self):
        """æ£€æŸ¥æœåŠ¡å™¨å’Œæ¨¡å‹çŠ¶æ€"""
        try:
            # æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦åœ¨çº¿
            if self.client.is_server_live():
                print(f"âœ… Triton Serveråœ¨çº¿")
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½
            if self.client.is_model_ready(self.model_name):
                print(f"âœ… OCRæ¨¡å‹å·²åŠ è½½å¹¶å¸¸é©»å†…å­˜")
                
                # è·å–æ¨¡å‹é…ç½®
                model_config = self.client.get_model_config(self.model_name)
                print(f"   æ¨¡å‹ç‰ˆæœ¬: {model_config.get('config', {}).get('version_policy', {})}")
                
        except Exception as e:
            print(f"âŒ æœåŠ¡å™¨æ£€æŸ¥å¤±è´¥: {e}")
            raise
    
    def process_regions(self, 
                       image: np.ndarray, 
                       regions: List[List[int]]) -> List[str]:
        """
        å¤„ç†å¤šä¸ªæ–‡æœ¬åŒºåŸŸ
        
        Args:
            image: è¾“å…¥å›¾åƒ
            regions: åŒºåŸŸåˆ—è¡¨ [[x1,y1,x2,y2], ...]
            
        Returns:
            OCRè¯†åˆ«ç»“æœåˆ—è¡¨
        """
        results = []
        
        for i, (x1, y1, x2, y2) in enumerate(regions):
            # è£å‰ªåŒºåŸŸ
            roi = image[y1:y2, x1:x2]
            
            if roi.size == 0:
                results.append("")
                continue
            
            # è°ƒç”¨Tritonæ¨ç†
            start_time = time.time()
            text = self._infer_single(roi)
            infer_time = (time.time() - start_time) * 1000
            
            results.append(text)
            print(f"   åŒºåŸŸ{i}: {text[:30]}... ({infer_time:.1f}ms)")
        
        return results
    
    def _infer_single(self, image: np.ndarray) -> str:
        """å•ä¸ªå›¾åƒæ¨ç†"""
        
        # å‡†å¤‡è¾“å…¥
        if self.use_grpc:
            inputs = [
                grpcclient.InferInput("image", image.shape, "UINT8")
            ]
            inputs[0].set_data_from_numpy(image)
            outputs = [grpcclient.InferRequestedOutput("text")]
        else:
            inputs = [
                httpclient.InferInput("image", image.shape, "UINT8")
            ]
            inputs[0].set_data_from_numpy(image)
            outputs = [httpclient.InferRequestedOutput("text")]
        
        # æ‰§è¡Œæ¨ç†
        response = self.client.infer(
            model_name=self.model_name,
            inputs=inputs,
            outputs=outputs
        )
        
        # è·å–ç»“æœ
        result = response.as_numpy("text")
        text = result[0].decode('utf-8') if isinstance(result[0], bytes) else str(result[0])
        
        return text
    
    def batch_infer(self, images: List[np.ndarray]) -> List[str]:
        """æ‰¹é‡æ¨ç†ï¼ˆåˆ©ç”¨Tritonçš„åŠ¨æ€æ‰¹å¤„ç†ï¼‰"""
        
        # TODO: å®ç°çœŸæ­£çš„æ‰¹å¤„ç†
        # Tritonä¼šè‡ªåŠ¨åˆå¹¶å¹¶å‘è¯·æ±‚è¿›è¡Œæ‰¹å¤„ç†
        results = []
        for img in images:
            text = self._infer_single(img)
            results.append(text)
        
        return results
    
    def get_model_stats(self):
        """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = self.client.get_inference_statistics(self.model_name)
            return stats
        except:
            return None

def demo_triton_ocr():
    """æ¼”ç¤ºTriton OCRçš„ä½¿ç”¨"""
    
    print("ğŸ­ Triton Server OCRæ¼”ç¤º")
    print("=" * 80)
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    def create_test_image():
        img = np.ones((400, 600, 3), dtype=np.uint8) * 255
        
        regions = []
        texts = ["Hello", "Triton", "OCR", "Server", "Test"]
        
        for i, text in enumerate(texts):
            x = (i % 3) * 180 + 20
            y = (i // 3) * 150 + 50
            
            cv2.putText(img, text, (x+10, y+30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)
            
            regions.append([x, y, x+160, y+60])
        
        return img, regions
    
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = TritonOCRClient(
            server_url="localhost:8000",
            model_name="ocr_model",
            use_grpc=False
        )
        
        print("\nğŸ“Š æµ‹è¯•1: å•æ¬¡è°ƒç”¨")
        print("-" * 40)
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨
        img1, regions1 = create_test_image()
        start = time.time()
        results1 = client.process_regions(img1, regions1)
        time1 = (time.time() - start) * 1000
        
        print(f"ç¬¬ä¸€æ¬¡è°ƒç”¨: {time1:.1f}ms")
        print(f"ç»“æœ: {results1}")
        
        print("\nğŸ“Š æµ‹è¯•2: æ¨¡æ‹Ÿè¿›ç¨‹é‡å¯åè°ƒç”¨")
        print("-" * 40)
        
        # åˆ›å»ºæ–°å®¢æˆ·ç«¯ï¼ˆæ¨¡æ‹Ÿæ–°è¿›ç¨‹ï¼‰
        del client
        client2 = TritonOCRClient(
            server_url="localhost:8000",
            model_name="ocr_model"
        )
        
        # ç¬¬äºŒæ¬¡è°ƒç”¨ - æ¨¡å‹ä»åœ¨Tritonä¸­ï¼
        img2, regions2 = create_test_image()
        start = time.time()
        results2 = client2.process_regions(img2, regions2)
        time2 = (time.time() - start) * 1000
        
        print(f"ç¬¬äºŒæ¬¡è°ƒç”¨: {time2:.1f}ms")
        print(f"ç»“æœ: {results2}")
        
        print("\nâœ… æµ‹è¯•ç»“æœ:")
        print(f"   é¦–æ¬¡è°ƒç”¨: {time1:.1f}ms")
        print(f"   é‡è¿è°ƒç”¨: {time2:.1f}ms")
        print(f"   ğŸ’¾ æ¨¡å‹åœ¨Triton Serverä¸­ä¿æŒåŠ è½½çŠ¶æ€")
        print(f"   âš¡ æ— éœ€é‡æ–°åŠ è½½ï¼Œäº«å—é¢„çƒ­æ¨¡å‹çš„é«˜é€Ÿå¤„ç†")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = client2.get_model_stats()
        if stats:
            print(f"\nğŸ“ˆ æ¨¡å‹ç»Ÿè®¡:")
            print(f"   {stats}")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("\nğŸ’¡ è¯·ç¡®ä¿Triton Serverå·²å¯åŠ¨:")
        print("   1. è¿è¡Œ: python triton_ocr_setup.py")
        print("   2. å¯åŠ¨Triton Server:")
        print("      docker run --gpus all -p 8000:8000 -p 8001:8001 \\")
        print("        -v ${PWD}/model_repository:/models \\")
        print("        nvcr.io/nvidia/tritonserver:latest \\")
        print("        tritonserver --model-repository=/models")

def benchmark_triton():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    print("\nğŸƒ Tritonæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    try:
        client = TritonOCRClient("localhost:8000")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_image = np.ones((32, 100, 3), dtype=np.uint8) * 255
        
        # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
        for num_calls in [1, 10, 100, 1000]:
            print(f"\næµ‹è¯• {num_calls} æ¬¡è°ƒç”¨:")
            
            start = time.time()
            for _ in range(num_calls):
                _ = client._infer_single(test_image)
            total_time = time.time() - start
            
            avg_time = (total_time / num_calls) * 1000
            throughput = num_calls / total_time
            
            print(f"   å¹³å‡æ—¶é—´: {avg_time:.2f}ms")
            print(f"   ååé‡: {throughput:.1f} req/s")
        
        print("\nâœ… åŸºå‡†æµ‹è¯•å®Œæˆ")
        print("ğŸ’¡ Tritonä¼˜åŠ¿:")
        print("   - æ¨¡å‹å¸¸é©»GPUæ˜¾å­˜ï¼Œé›¶åŠ è½½æ—¶é—´")
        print("   - åŠ¨æ€æ‰¹å¤„ç†ä¼˜åŒ–")
        print("   - å¤šGPUè´Ÿè½½å‡è¡¡")
        print("   - ç”Ÿäº§çº§ç¨³å®šæ€§")
        
    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    demo_triton_ocr()
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_triton()
    
    print("\n" + "=" * 80)
    print("ğŸ‰ Triton Serverå®Œç¾å®ç°éœ€æ±‚:")
    print("âœ… æ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼Œå¸¸é©»GPUæ˜¾å­˜")
    print("âœ… å®¢æˆ·ç«¯è¿›ç¨‹ç»“æŸä¸å½±å“æ¨¡å‹")
    print("âœ… æ”¯æŒå¤šå®¢æˆ·ç«¯å¹¶å‘è®¿é—®")
    print("âœ… ä¼ä¸šçº§æ€§èƒ½å’Œç¨³å®šæ€§")
    print("=" * 80)
